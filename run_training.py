#!/usr/bin/env python3
"""
æ—¶åºè€¦åˆçº¦æŸè®­ç»ƒè„šæœ¬ - å¸¦ä¾èµ–æ£€æŸ¥
"""

import sys
import subprocess

# æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–
def check_and_install_dependencies():
    """æ£€æŸ¥ä¾èµ–å¹¶å°è¯•å®‰è£…"""
    dependencies = {
        'numpy': 'numpy',
        'torch': 'torch',
        'gurobipy': 'gurobipy',
        'pypower': 'PYPOWER'
    }
    
    missing = []
    for import_name, package_name in dependencies.items():
        try:
            __import__(import_name)
            print(f"âœ“ {import_name} å·²å®‰è£…")
        except ImportError:
            missing.append(package_name)
            print(f"âœ— {import_name} æœªå®‰è£…")
    
    if missing:
        print(f"\nç¼ºå°‘ä¾èµ–: {', '.join(missing)}")
        response = input("æ˜¯å¦è‡ªåŠ¨å®‰è£…ï¼Ÿ(y/n): ")
        if response.lower() == 'y':
            print("æ­£åœ¨å®‰è£…...")
            subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + missing)
            print("âœ“ ä¾èµ–å®‰è£…å®Œæˆ")
            return True
        else:
            print("è¯·æ‰‹åŠ¨å®‰è£…ä¾èµ–åé‡è¯•")
            return False
    return True

if not check_and_install_dependencies():
    sys.exit(1)

# å¯¼å…¥æ¨¡å—
import numpy as np
import json
from pathlib import Path

# æ·»åŠ æºç è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / 'src'))

try:
    import pypower.case14 as case14
    from uc_NN_subproblem import (
        train_dual_predictor_from_data,
        train_subproblem_surrogate_from_data,
        ActiveSetReader
    )
except ImportError as e:
    print(f"æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)

def main():
    print("=" * 70)
    print("ğŸš€ æ—¶åºè€¦åˆçº¦æŸè®­ç»ƒè„šæœ¬")
    print("=" * 70)
    
    # 1. é€‰æ‹©æ•°æ®æ–‡ä»¶
    result_dir = Path(__file__).parent / 'result'
    json_files = list(result_dir.glob('active_sets_*.json'))
    
    if not json_files:
        print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        return
    
    print(f"\nğŸ“¦ æ‰¾åˆ° {len(json_files)} ä¸ªæ•°æ®æ–‡ä»¶:")
    for i, f in enumerate(json_files[:5]):
        print(f"  {i+1}. {f.name}")
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶
    data_file = json_files[0]
    print(f"\nâœ“ ä½¿ç”¨æ•°æ®æ–‡ä»¶: {data_file.name}")
    
    # 2. åŠ è½½æ•°æ®
    print("\nğŸ“Š åŠ è½½æ•°æ®...")
    with open(data_file, 'r') as f:
        data = json.load(f)
    
    all_samples = data['all_samples']
    n_samples = len(all_samples)
    print(f"âœ“ åŠ è½½ {n_samples} ä¸ªæ ·æœ¬")
    
    # é™åˆ¶æ ·æœ¬æ•°é‡åŠ å¿«æµ‹è¯•
    max_samples = 10
    if n_samples > max_samples:
        print(f"  (é™åˆ¶ä¸ºå‰{max_samples}ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€Ÿæµ‹è¯•)")
        all_samples = all_samples[:max_samples]
    
    # è½¬æ¢æ•°æ®æ ¼å¼ï¼šlist -> numpy array
    print("  è½¬æ¢æ•°æ®æ ¼å¼...")
    for sample in all_samples:
        sample['pd_data'] = np.array(sample['pd_data'])
    print("  âœ“ æ•°æ®æ ¼å¼è½¬æ¢å®Œæˆ")
    
    # 3. åŠ è½½PyPoweræ¡ˆä¾‹
    print("\nğŸ”Œ åŠ è½½PyPoweræ¡ˆä¾‹...")
    ppc = case14.case14()
    print(f"âœ“ case14: {ppc['gen'].shape[0]}ä¸ªæœºç»„, {ppc['bus'].shape[0]}ä¸ªèŠ‚ç‚¹")
    
    # 4. è®­ç»ƒå¯¹å¶å˜é‡é¢„æµ‹å™¨
    print("\n" + "=" * 70)
    print("ç¬¬1é˜¶æ®µï¼šè®­ç»ƒå¯¹å¶å˜é‡é¢„æµ‹å™¨")
    print("=" * 70)
    
    try:
        lambda_predictor = train_dual_predictor_from_data(
            ppc, all_samples, T_delta=1.0,
            num_epochs=10,
            batch_size=min(4, len(all_samples)),
            save_path='result/dual_predictor.pth'
        )
        print("\nâœ… å¯¹å¶å˜é‡é¢„æµ‹å™¨è®­ç»ƒå®Œæˆ")
    except Exception as e:
        print(f"\nâŒ å¯¹å¶å˜é‡é¢„æµ‹å™¨è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 5. è®­ç»ƒæ—¶åºè€¦åˆä»£ç†çº¦æŸ
    print("\n" + "=" * 70)
    print("ç¬¬2é˜¶æ®µï¼šè®­ç»ƒæ—¶åºè€¦åˆä»£ç†çº¦æŸ")
    print("=" * 70)
    
    # é€‰æ‹©ä¸€ä¸ªæœºç»„è¿›è¡Œè®­ç»ƒ
    unit_id = 0
    print(f"\nğŸ“ è®­ç»ƒæœºç»„ {unit_id}...")
    
    try:
        trainer = train_subproblem_surrogate_from_data(
            ppc, all_samples, unit_id=unit_id,
            T_delta=1.0, lambda_predictor=lambda_predictor,
            max_iter=5,  # å¿«é€Ÿæµ‹è¯•ç”¨è¾ƒå°‘è¿­ä»£
            nn_epochs=5,
            save_path=f'result/temporal_coupling_unit{unit_id}.pth'
        )
        print(f"\nâœ… æœºç»„{unit_id}è®­ç»ƒå®Œæˆ")
        
        # 6. éªŒè¯ç»“æœ
        print("\n" + "=" * 70)
        print("éªŒè¯ç»“æœ")
        print("=" * 70)
        
        T = trainer.T
        print(f"\nğŸ“ å‚æ•°å½¢çŠ¶æ£€æŸ¥:")
        print(f"  alpha_values: {trainer.alpha_values.shape} (æœŸæœ›: {(len(all_samples), T-1)})")
        print(f"  beta_values: {trainer.beta_values.shape} (æœŸæœ›: {(len(all_samples), T-1)})")
        print(f"  gamma_values: {trainer.gamma_values.shape} (æœŸæœ›: {(len(all_samples), T-1)})")
        
        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„çº¦æŸ
        print(f"\nğŸ¯ æ ·æœ¬0çš„æ—¶åºè€¦åˆçº¦æŸ:")
        for t in range(min(5, T-1)):  # åªæ˜¾ç¤ºå‰5ä¸ª
            alpha_t = trainer.alpha_values[0, t]
            beta_t = trainer.beta_values[0, t]
            gamma_t = trainer.gamma_values[0, t]
            x_t = trainer.x[0, t]
            x_t1 = trainer.x[0, t+1]
            lhs = alpha_t * x_t + beta_t * x_t1
            viol = max(0, lhs - gamma_t)
            print(f"  t={t}: {alpha_t:.3f}*x[{t}] + {beta_t:.3f}*x[{t+1}] â‰¤ {gamma_t:.3f}")
            print(f"        lhs={lhs:.3f}, viol={viol:.6f}, x[{t}]={x_t:.3f}, x[{t+1}]={x_t1:.3f}")
        
        # è®¡ç®—æ•´æ•°æ€§
        x_vals = trainer.x[0]
        integrality = np.sum(x_vals * (1 - x_vals))
        print(f"\nğŸ“ æ•´æ•°æ€§æŒ‡æ ‡: {integrality:.6f} (è¶Šå°è¶Šå¥½ï¼Œ0=å®Œå…¨æ•´æ•°)")
        
        print("\n" + "=" * 70)
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        print("=" * 70)
        
        # ä¿å­˜ä½ç½®
        save_path = Path('result') / f'temporal_coupling_unit{unit_id}.pth'
        print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
