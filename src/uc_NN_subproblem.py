import numpy as np
import gurobipy as gp
from gurobipy import GRB
import sys
import io
import time
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import warnings

import os
import matplotlib.pyplot as plt

# å°è¯•å¯¼å…¥PyTorch
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("è­¦å‘Š: PyTorchæœªå®‰è£…ï¼Œå°†æ— æ³•ä½¿ç”¨ç¥ç»ç½‘ç»œåŠŸèƒ½", flush=True)

# å¯¼å…¥å¿…è¦çš„å·¥å…·å‡½æ•°
from pypower.ext2int import ext2int
from pypower.idx_gen import GEN_BUS, PMIN, PMAX

# å¯¼å…¥pypowerç”¨äºæµ‹è¯•
try:
    import pypower
    import pypower.case39
    import pypower.case14
    import pypower.case30
    PYPOWER_AVAILABLE = True
except ImportError:
    PYPOWER_AVAILABLE = False
    print("è­¦å‘Š: pypoweræœªå®‰è£…ï¼Œæµ‹è¯•ä»£ç å¯èƒ½æ— æ³•è¿è¡Œ", flush=True)

# è®¾ç½®è¾“å‡ºç¼“å†²
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', line_buffering=True)


# ========================== æ•°æ®åŠ è½½å·¥å…· ==========================

class ActiveSetReader:
    """è¯»å–å’Œè§£ææ´»åŠ¨é›†JSONæ–‡ä»¶çš„å·¥å…·ç±»"""
    
    def __init__(self, json_filepath: str):
        self.json_filepath = Path(json_filepath)
        self.data = self._load_json()
        
    def _load_json(self) -> Dict:
        try:
            with open(self.json_filepath, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            raise FileNotFoundError(f"JSONæ–‡ä»¶æœªæ‰¾åˆ°: {self.json_filepath}")
        except json.JSONDecodeError:
            raise ValueError(f"JSONæ–‡ä»¶æ ¼å¼é”™è¯¯: {self.json_filepath}")
    
    def get_sample_data(self, sample_id: int) -> Optional[Dict]:
        samples = self.data.get('all_samples', [])
        if 0 <= sample_id < len(samples):
            return samples[sample_id]
        return None
    
    def get_total_samples_count(self) -> int:
        return len(self.data.get('all_samples', []))
    
    def load_all_samples(self) -> List[Dict]:
        all_samples_data = []
        total_samples = self.get_total_samples_count()
        
        for sample_id in range(total_samples):
            try:
                active_constraints, active_variables, pd_data = self.extract_active_constraints_and_variables(sample_id)
                unit_commitment = self.get_unit_commitment_matrix(sample_id)
                
                sample_data = {
                    'sample_id': sample_id,
                    'active_constraints': active_constraints,
                    'active_variables': active_variables,
                    'pd_data': pd_data,
                    'unit_commitment_matrix': unit_commitment
                }
                
                sample = self.get_sample_data(sample_id)
                if sample and 'lambda' in sample:
                    sample_data['lambda'] = sample['lambda']
                
                all_samples_data.append(sample_data)
                    
            except Exception as e:
                print(f"åŠ è½½æ ·æœ¬ {sample_id} æ—¶å‡ºé”™: {e}", flush=True)
                all_samples_data.append({
                    'sample_id': sample_id,
                    'error': str(e)
                })
        
        return all_samples_data
    
    def extract_active_constraints_and_variables(self, sample_id: int) -> Tuple[List, List, np.ndarray]:
        sample = self.get_sample_data(sample_id)
        if sample is None:
            return [], [], np.array([])
        
        active_set = sample.get('active_set', [])
        pd_data = np.array(sample.get('pd_data', []))
        
        active_constraints = []
        active_variables = []
        
        for item in active_set:
            if isinstance(item, list) and len(item) == 2:
                if isinstance(item[0], list) and len(item[0]) == 2:
                    active_variables.append({
                        'type': 'binary_variable',
                        'unit_id': item[0][0],
                        'time_slot': item[0][1],
                        'value': item[1]
                    })
                else:
                    active_constraints.append({
                        'constraint_id': item[0],
                        'dual_value': item[1]
                    })
        
        return active_constraints, active_variables, pd_data
    
    def get_unit_commitment_matrix(self, sample_id: int) -> np.ndarray:
        _, active_variables, _ = self.extract_active_constraints_and_variables(sample_id)
        
        if not active_variables:
            return np.array([])
        
        binary_vars = [v for v in active_variables if v.get('type') == 'binary_variable']
        if not binary_vars:
            return np.array([])
            
        max_unit = max([v['unit_id'] for v in binary_vars]) + 1
        max_time = max([v['time_slot'] for v in binary_vars]) + 1
        
        unit_commitment = np.zeros((max_unit, max_time), dtype=int)
        for var in binary_vars:
            unit_commitment[var['unit_id'], var['time_slot']] = var['value']
        
        return unit_commitment


def load_active_set_from_json(json_filepath: str, sample_id: Optional[int] = None):
    """ä»JSONæ–‡ä»¶åŠ è½½æ´»åŠ¨é›†æ•°æ®"""
    reader = ActiveSetReader(json_filepath)
    
    if sample_id is not None:
        active_constraints, active_variables, pd_data = reader.extract_active_constraints_and_variables(sample_id)
        unit_commitment = reader.get_unit_commitment_matrix(sample_id)
        return {
            'sample_id': sample_id,
            'active_constraints': active_constraints,
            'active_variables': active_variables,
            'pd_data': pd_data,
            'unit_commitment_matrix': unit_commitment
        }
    else:
        return reader.load_all_samples()


# ========================== ç¬¬ä¸€éƒ¨åˆ†ï¼šå¯¹å¶å˜é‡é¢„æµ‹ç½‘ç»œ ==========================

class DualVariablePredictorNet(nn.Module):
    """
    ä»Pdæ•°æ®é¢„æµ‹å¯¹å¶å˜é‡çš„ç¥ç»ç½‘ç»œ
    
    è¾“å…¥: Pdæ•°æ®å±•å¹³ (nb * T,)
    è¾“å‡º: åŠŸç‡å¹³è¡¡çº¦æŸçš„å¯¹å¶å˜é‡ Î» (T,)
    """
    
    def __init__(self, input_dim: int, output_dim: int, hidden_dims: List[int] = None):
        super(DualVariablePredictorNet, self).__init__()
        
        if hidden_dims is None:
            hidden_dims = [256, 512, 256]
        
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.1))
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        self.network = nn.Sequential(*layers)
        self._init_weights()
    
    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, x):
        return self.network(x)


class DualVariablePredictorTrainer:
    """
    å¯¹å¶å˜é‡é¢„æµ‹ç½‘ç»œçš„ç‹¬ç«‹è®­ç»ƒå™¨
    
    åŠŸèƒ½ï¼šè®­ç»ƒç¥ç»ç½‘ç»œä»Pdæ•°æ®é¢„æµ‹åŠŸç‡å¹³è¡¡çº¦æŸçš„å¯¹å¶å˜é‡Î»
    è®­ç»ƒæ–¹å¼ï¼šç›‘ç£å­¦ä¹ ï¼ˆMSEæŸå¤±ï¼‰
    """
    
    def __init__(self, ppc, active_set_data, T_delta, device=None):
        self.ppc = ppc
        ppc_int = ext2int(ppc)
        self.baseMVA = ppc_int['baseMVA']
        self.bus = ppc_int['bus']
        self.gen = ppc_int['gen']
        self.branch = ppc_int['branch']
        self.gencost = ppc_int['gencost']
        self.n_samples = len(active_set_data)
        self.T_delta = T_delta
        
        if isinstance(active_set_data, list):
            self.T = active_set_data[0]['pd_data'].shape[1]
        else:
            self.T = active_set_data['pd_data'].shape[1]
            
        self.ng = self.gen.shape[0]
        self.nb = self.bus.shape[0]
        self.active_set_data = active_set_data
        
        # è®¾ç½®è®¾å¤‡
        if device is None:
            if TORCH_AVAILABLE and torch.cuda.is_available():
                self.device = torch.device('cuda')
            else:
                self.device = torch.device('cpu')
        else:
            self.device = device
        
        # è¾“å…¥è¾“å‡ºç»´åº¦
        self.input_dim = self.nb * self.T
        self.output_dim = self.T
        
        # åˆå§‹åŒ–ç½‘ç»œ
        if TORCH_AVAILABLE:
            self.network = DualVariablePredictorNet(
                input_dim=self.input_dim,
                output_dim=self.output_dim
            ).to(self.device)
            self.optimizer = optim.Adam(self.network.parameters(), lr=1e-3)
        
        # æ±‚è§£åŸå§‹é—®é¢˜è·å–å¯¹å¶å˜é‡çœŸå€¼
        self.lambda_true = self._solve_for_true_dual_variables()
        
        print(f"âœ“ å¯¹å¶å˜é‡é¢„æµ‹è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ", flush=True)
        print(f"  - è¾“å…¥ç»´åº¦: {self.input_dim}, è¾“å‡ºç»´åº¦: {self.output_dim}", flush=True)
    
    def _solve_for_true_dual_variables(self) -> np.ndarray:
        """æ±‚è§£UCé—®é¢˜è·å–åŠŸç‡å¹³è¡¡çº¦æŸçš„å¯¹å¶å˜é‡çœŸå€¼"""
        lambda_true = []
        
        for sample_id in range(self.n_samples):
            Pd = self.active_set_data[sample_id]['pd_data']
            model = gp.Model('uc_for_dual')
            model.Params.OutputFlag = 0
            
            # å˜é‡
            pg = model.addVars(self.ng, self.T, lb=0, name='pg')
            x = model.addVars(self.ng, self.T, vtype=GRB.BINARY, name='x')
            coc = model.addVars(self.ng, self.T-1, lb=0, name='coc')
            cpower = model.addVars(self.ng, self.T, lb=0, name='cpower')
            
            # åŠŸç‡å¹³è¡¡çº¦æŸ
            for t in range(self.T):
                model.addConstr(
                    gp.quicksum(pg[g, t] for g in range(self.ng)) == np.sum(Pd[:, t]),
                    name=f'power_balance_{t}'
                )
            
            # å‘ç”µä¸Šä¸‹é™çº¦æŸ
            for g in range(self.ng):
                for t in range(self.T):
                    model.addConstr(pg[g, t] >= self.gen[g, PMIN] * x[g, t], name=f'pg_lower_{g}_{t}')
                    model.addConstr(pg[g, t] <= self.gen[g, PMAX] * x[g, t], name=f'pg_upper_{g}_{t}')
            
            # çˆ¬å¡çº¦æŸ
            Ru = 0.4 * self.gen[:, PMAX] / self.T_delta
            Rd = 0.4 * self.gen[:, PMAX] / self.T_delta
            for g in range(self.ng):
                for t in range(1, self.T):
                    model.addConstr(pg[g, t] - pg[g, t-1] <= Ru[g] * x[g, t-1] + self.gen[g, PMAX] * (1 - x[g, t-1]))
                    model.addConstr(pg[g, t-1] - pg[g, t] <= Rd[g] * x[g, t] + self.gen[g, PMAX] * (1 - x[g, t]))
            
            # æœ€å°å¼€å…³æœºæ—¶é—´çº¦æŸ
            Ton = min(4, self.T)
            Toff = min(4, self.T)
            for g in range(self.ng):
                for tau in range(1, Ton+1):
                    for t1 in range(self.T - tau):
                        model.addConstr(x[g, t1+1] - x[g, t1] <= x[g, t1+tau])
                for tau in range(1, Toff+1):
                    for t1 in range(self.T - tau):
                        model.addConstr(-x[g, t1+1] + x[g, t1] <= 1 - x[g, t1+tau])
            
            # å¯åœæˆæœ¬
            start_cost = self.gencost[:, 1]
            shut_cost = self.gencost[:, 2]
            for t in range(1, self.T):
                for g in range(self.ng):
                    model.addConstr(coc[g, t-1] >= start_cost[g] * (x[g, t] - x[g, t-1]))
                    model.addConstr(coc[g, t-1] >= shut_cost[g] * (x[g, t-1] - x[g, t]))
            
            # å‘ç”µæˆæœ¬
            for t in range(self.T):
                for g in range(self.ng):
                    model.addConstr(cpower[g, t] >= self.gencost[g, -2]/self.T_delta * pg[g, t] + 
                                  self.gencost[g, -1]/self.T_delta * x[g, t])
            
            # ç›®æ ‡å‡½æ•°
            obj = (gp.quicksum(cpower[g, t] for g in range(self.ng) for t in range(self.T)) +
                   gp.quicksum(coc[g, t] for g in range(self.ng) for t in range(self.T-1)))
            model.setObjective(obj, GRB.MINIMIZE)
            model.optimize()
            
            if model.status == GRB.OPTIMAL:
                lambda_sample = np.zeros(self.T)
                for t in range(self.T):
                    constr = model.getConstrByName(f'power_balance_{t}')
                    if constr is not None:
                        try:
                            lambda_sample[t] = constr.Pi
                        except:
                            lambda_sample[t] = 0.0
                lambda_true.append(lambda_sample)
            else:
                lambda_true.append(np.zeros(self.T))
        
        return np.array(lambda_true)
    
    def _extract_features(self, sample_id: int) -> np.ndarray:
        """æå–Pdæ•°æ®ä½œä¸ºç‰¹å¾"""
        pd_data = self.active_set_data[sample_id]['pd_data']
        return pd_data.flatten()
    
    def train(self, num_epochs: int = 100, batch_size: int = 8):
        """è®­ç»ƒå¯¹å¶å˜é‡é¢„æµ‹ç½‘ç»œ"""
        if not TORCH_AVAILABLE:
            print("è­¦å‘Š: PyTorchä¸å¯ç”¨", flush=True)
            return
        
        print(f"å¼€å§‹è®­ç»ƒå¯¹å¶å˜é‡é¢„æµ‹ç½‘ç»œ (epochs={num_epochs})...", flush=True)
        
        # å‡†å¤‡æ•°æ®
        X = np.array([self._extract_features(i) for i in range(self.n_samples)])
        Y = self.lambda_true
        
        X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)
        Y_tensor = torch.tensor(Y, dtype=torch.float32, device=self.device)
        
        dataset = torch.utils.data.TensorDataset(X_tensor, Y_tensor)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        criterion = nn.MSELoss()
        self.network.train()
        
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            for batch_X, batch_Y in dataloader:
                self.optimizer.zero_grad()
                pred = self.network(batch_X)
                loss = criterion(pred, batch_Y)
                loss.backward()
                self.optimizer.step()
                epoch_loss += loss.item() * batch_X.size(0)
            
            epoch_loss /= len(dataset)
            
            if (epoch + 1) % 20 == 0:
                print(f"  Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.6f}", flush=True)
        
        print(f"âœ“ å¯¹å¶å˜é‡é¢„æµ‹ç½‘ç»œè®­ç»ƒå®Œæˆ", flush=True)
    
    def predict(self, pd_data: np.ndarray) -> np.ndarray:
        """é¢„æµ‹å¯¹å¶å˜é‡"""
        if not TORCH_AVAILABLE:
            raise RuntimeError("PyTorchä¸å¯ç”¨")
        
        self.network.eval()
        pd_flat = pd_data.flatten()
        pd_tensor = torch.tensor(pd_flat, dtype=torch.float32, device=self.device)
        
        with torch.no_grad():
            lambda_pred = self.network(pd_tensor.unsqueeze(0)).squeeze(0)
        
        return lambda_pred.cpu().numpy()
    
    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        if TORCH_AVAILABLE:
            # #region agent log
            import json as _json_debug; _log_path = r'd:\0-python_workspace\branchandcut\.cursor\debug.log'; _log_data = {"location": "uc_NN_subproblem.py:save:427", "message": "DualVariablePredictorTrainer.save called", "data": {"filepath": filepath, "cwd": os.getcwd(), "abs_filepath": os.path.abspath(filepath)}, "timestamp": int(__import__('time').time()*1000), "sessionId": "debug-session", "hypothesisId": "A"}; open(_log_path, 'a', encoding='utf-8').write(_json_debug.dumps(_log_data) + '\n')
            # #endregion
            dirpath = os.path.dirname(os.path.abspath(filepath))
            # #region agent log
            _log_data2 = {"location": "uc_NN_subproblem.py:save:430", "message": "Checking dirpath", "data": {"dirpath": dirpath, "exists": os.path.exists(dirpath)}, "timestamp": int(__import__('time').time()*1000), "sessionId": "debug-session", "hypothesisId": "B"}; open(_log_path, 'a', encoding='utf-8').write(_json_debug.dumps(_log_data2) + '\n')
            # #endregion
            if dirpath and not os.path.exists(dirpath):
                # #region agent log
                _log_data3 = {"location": "uc_NN_subproblem.py:save:433", "message": "Creating directory", "data": {"dirpath": dirpath}, "timestamp": int(__import__('time').time()*1000), "sessionId": "debug-session", "hypothesisId": "C"}; open(_log_path, 'a', encoding='utf-8').write(_json_debug.dumps(_log_data3) + '\n')
                # #endregion
                os.makedirs(dirpath, exist_ok=True)
            torch.save({
                'network_state_dict': self.network.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict()
            }, filepath)
            print(f"âœ“ å¯¹å¶é¢„æµ‹æ¨¡å‹å·²ä¿å­˜: {filepath}", flush=True)
    
    def load(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        if TORCH_AVAILABLE:
            state = torch.load(filepath, map_location=self.device)
            self.network.load_state_dict(state['network_state_dict'])
            self.optimizer.load_state_dict(state['optimizer_state_dict'])
            print(f"âœ“ å¯¹å¶é¢„æµ‹æ¨¡å‹å·²åŠ è½½: {filepath}", flush=True)


# ========================== ç¬¬äºŒéƒ¨åˆ†ï¼šå­é—®é¢˜ä»£ç†çº¦æŸè®­ç»ƒï¼ˆBCDæ–¹å¼ï¼‰ ==========================

class SubproblemSurrogateNet(nn.Module):
    """
    å•æœºç»„å­é—®é¢˜çš„ä»£ç†çº¦æŸç½‘ç»œ
    
    è¾“å…¥: Pdæ•°æ® + å¯¹å¶å˜é‡Î» (pd_dim + T)
    è¾“å‡º: ä»£ç†çº¦æŸå‚æ•° (alpha, beta)
          çº¦æŸå½¢å¼: sum_t(alpha_t * x_t) <= beta
    """
    
    def __init__(self, input_dim: int, T: int, hidden_dims: List[int] = None):
        super(SubproblemSurrogateNet, self).__init__()
        
        if hidden_dims is None:
            hidden_dims = [128, 256, 128]
        
        # alphaç½‘ç»œï¼šé¢„æµ‹çº¦æŸç³»æ•°
        alpha_layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            alpha_layers.append(nn.Linear(prev_dim, hidden_dim))
            alpha_layers.append(nn.ReLU())
            alpha_layers.append(nn.Dropout(0.1))
            prev_dim = hidden_dim
        alpha_layers.append(nn.Linear(prev_dim, T))
        self.alpha_net = nn.Sequential(*alpha_layers)
        
        # betaç½‘ç»œï¼šé¢„æµ‹å³ç«¯é¡¹ï¼ˆæ ‡é‡ï¼‰
        beta_layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            beta_layers.append(nn.Linear(prev_dim, hidden_dim))
            beta_layers.append(nn.ReLU())
            beta_layers.append(nn.Dropout(0.1))
            prev_dim = hidden_dim
        beta_layers.append(nn.Linear(prev_dim, 1))
        beta_layers.append(nn.Softplus())  # ç¡®ä¿betaéè´Ÿ
        self.beta_net = nn.Sequential(*beta_layers)
        
        self._init_weights()
    
    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, x):
        alpha = self.alpha_net(x)
        beta = self.beta_net(x)
        return alpha, beta


class SubproblemSurrogateTrainer:
    """
    å•æœºç»„å­é—®é¢˜ä»£ç†çº¦æŸçš„BCDè®­ç»ƒå™¨
    
    è®­ç»ƒæ–¹å¼ä¸uc_NN_BCD.pyä¸€è‡´ï¼š
    1. iter_with_primal_block: å›ºå®šä»£ç†çº¦æŸå‚æ•°ï¼Œæ±‚è§£å­é—®é¢˜æ›´æ–°åŸå§‹å˜é‡(pg, x)
    2. iter_with_dual_block: å›ºå®šåŸå§‹å˜é‡ï¼Œæ±‚è§£å¯¹å¶é—®é¢˜æ›´æ–°å¯¹å¶å˜é‡(mu)
    3. iter_with_surrogate_nn: ä½¿ç”¨å¯å¾®åˆ†losså‡½æ•°è®­ç»ƒç¥ç»ç½‘ç»œæ›´æ–°ä»£ç†çº¦æŸå‚æ•°
    
    æ‹‰æ ¼æœ—æ—¥æ¾å¼›å­é—®é¢˜å½¢å¼ï¼š
        min  cost_g(pg, x) - Î»áµ€ Ã— pg + sum_t(mu * max(0, alpha_t * x_t - beta))
        s.t. pg_min * x <= pg <= pg_max * x
             çˆ¬å¡çº¦æŸ
             æœ€å°å¼€å…³æœºæ—¶é—´çº¦æŸ
             å¯åœæˆæœ¬çº¦æŸ
    """
    
    def __init__(self, ppc, active_set_data, T_delta, unit_id: int, 
                 lambda_predictor: DualVariablePredictorTrainer = None, device=None):
        """
        åˆå§‹åŒ–å•æœºç»„å­é—®é¢˜ä»£ç†çº¦æŸè®­ç»ƒå™¨
        
        Args:
            ppc: PyPoweræ¡ˆä¾‹æ•°æ®
            active_set_data: æ´»åŠ¨é›†æ•°æ®
            T_delta: æ—¶é—´é—´éš”
            unit_id: æœºç»„ç´¢å¼•
            lambda_predictor: å·²è®­ç»ƒçš„å¯¹å¶å˜é‡é¢„æµ‹å™¨ï¼ˆå¯é€‰ï¼‰
            device: è®¡ç®—è®¾å¤‡
        """
        self.ppc = ppc
        ppc_int = ext2int(ppc)
        self.baseMVA = ppc_int['baseMVA']
        self.bus = ppc_int['bus']
        self.gen = ppc_int['gen']
        self.branch = ppc_int['branch']
        self.gencost = ppc_int['gencost']
        self.n_samples = len(active_set_data)
        self.T_delta = T_delta
        self.unit_id = unit_id
        
        if isinstance(active_set_data, list):
            self.T = active_set_data[0]['pd_data'].shape[1]
        else:
            self.T = active_set_data['pd_data'].shape[1]
            
        self.ng = self.gen.shape[0]
        self.nb = self.bus.shape[0]
        self.active_set_data = active_set_data
        
        # å¯¹å¶å˜é‡é¢„æµ‹å™¨
        self.lambda_predictor = lambda_predictor
        
        # è®¾å¤‡
        if device is None:
            if TORCH_AVAILABLE and torch.cuda.is_available():
                self.device = torch.device('cuda')
            else:
                self.device = torch.device('cpu')
        else:
            self.device = device
        
        # BCDè¿­ä»£å‚æ•°
        self.rho_primal = 1e-2
        self.rho_dual = 1e-2
        self.rho_opt = 1e-2
        self.gamma = 1e-1
        self.mu_lower_bound = 0.1
        self.iter_number = 0
        
        # åˆå§‹åŒ–åŸå§‹å˜é‡å’Œå¯¹å¶å˜é‡å­˜å‚¨
        self.pg = np.zeros((self.n_samples, self.T))
        self.x = np.zeros((self.n_samples, self.T))
        self.coc = np.zeros((self.n_samples, self.T-1))
        self.cpower = np.zeros((self.n_samples, self.T))
        self.mu = np.ones((self.n_samples,)) * self.mu_lower_bound  # ä»£ç†çº¦æŸçš„å¯¹å¶å˜é‡ï¼ˆæ ‡é‡ï¼‰
        
        # è·å–å¯¹å¶å˜é‡Î»
        self.lambda_vals = self._get_lambda_values()
        
        # åˆå§‹åŒ–ä»£ç†çº¦æŸå‚æ•°
        self.alpha_values = np.zeros((self.n_samples, self.T))
        self.beta_values = np.ones((self.n_samples,))
        
        # åˆå§‹åŒ–ç¥ç»ç½‘ç»œ
        if TORCH_AVAILABLE:
            self._init_neural_network()
        
        # åˆå§‹åŒ–æ±‚è§£
        self._initialize_solve()
        
        print(f"âœ“ æœºç»„{unit_id}å­é—®é¢˜ä»£ç†çº¦æŸè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ", flush=True)
    
    def _get_lambda_values(self) -> np.ndarray:
        """è·å–å¯¹å¶å˜é‡Î»"""
        if self.lambda_predictor is not None:
            # ä½¿ç”¨é¢„æµ‹å™¨
            lambda_vals = []
            for sample_id in range(self.n_samples):
                pd_data = self.active_set_data[sample_id]['pd_data']
                lambda_pred = self.lambda_predictor.predict(pd_data)
                lambda_vals.append(lambda_pred)
            return np.array(lambda_vals)
        else:
            # ä½¿ç”¨çœŸå€¼ï¼ˆéœ€è¦å…ˆæ±‚è§£åŸé—®é¢˜ï¼‰
            return self._solve_for_lambda()
    
    def _solve_for_lambda(self) -> np.ndarray:
        """æ±‚è§£åŸé—®é¢˜è·å–Î»"""
        lambda_vals = []
        for sample_id in range(self.n_samples):
            Pd = self.active_set_data[sample_id]['pd_data']
            model = gp.Model('uc_for_lambda')
            model.Params.OutputFlag = 0
            
            pg = model.addVars(self.ng, self.T, lb=0, name='pg')
            x = model.addVars(self.ng, self.T, vtype=GRB.BINARY, name='x')
            coc = model.addVars(self.ng, self.T-1, lb=0, name='coc')
            cpower = model.addVars(self.ng, self.T, lb=0, name='cpower')
            
            # åŠŸç‡å¹³è¡¡çº¦æŸ
            for t in range(self.T):
                model.addConstr(gp.quicksum(pg[g, t] for g in range(self.ng)) == np.sum(Pd[:, t]),
                              name=f'power_balance_{t}')
            
            # å…¶ä»–çº¦æŸï¼ˆç®€åŒ–ï¼‰
            for g in range(self.ng):
                for t in range(self.T):
                    model.addConstr(pg[g, t] >= self.gen[g, PMIN] * x[g, t])
                    model.addConstr(pg[g, t] <= self.gen[g, PMAX] * x[g, t])
            
            Ru = 0.4 * self.gen[:, PMAX] / self.T_delta
            Rd = 0.4 * self.gen[:, PMAX] / self.T_delta
            for g in range(self.ng):
                for t in range(1, self.T):
                    model.addConstr(pg[g, t] - pg[g, t-1] <= Ru[g] * x[g, t-1] + self.gen[g, PMAX] * (1 - x[g, t-1]))
                    model.addConstr(pg[g, t-1] - pg[g, t] <= Rd[g] * x[g, t] + self.gen[g, PMAX] * (1 - x[g, t]))
            
            Ton = min(4, self.T)
            Toff = min(4, self.T)
            for g in range(self.ng):
                for tau in range(1, Ton+1):
                    for t1 in range(self.T - tau):
                        model.addConstr(x[g, t1+1] - x[g, t1] <= x[g, t1+tau])
                for tau in range(1, Toff+1):
                    for t1 in range(self.T - tau):
                        model.addConstr(-x[g, t1+1] + x[g, t1] <= 1 - x[g, t1+tau])
            
            start_cost = self.gencost[:, 1]
            shut_cost = self.gencost[:, 2]
            for t in range(1, self.T):
                for g in range(self.ng):
                    model.addConstr(coc[g, t-1] >= start_cost[g] * (x[g, t] - x[g, t-1]))
                    model.addConstr(coc[g, t-1] >= shut_cost[g] * (x[g, t-1] - x[g, t]))
            
            for t in range(self.T):
                for g in range(self.ng):
                    model.addConstr(cpower[g, t] >= self.gencost[g, -2]/self.T_delta * pg[g, t] + 
                                  self.gencost[g, -1]/self.T_delta * x[g, t])
            
            obj = (gp.quicksum(cpower[g, t] for g in range(self.ng) for t in range(self.T)) +
                   gp.quicksum(coc[g, t] for g in range(self.ng) for t in range(self.T-1)))
            model.setObjective(obj, GRB.MINIMIZE)
            model.optimize()
            
            if model.status == GRB.OPTIMAL:
                lambda_sample = np.zeros(self.T)
                for t in range(self.T):
                    constr = model.getConstrByName(f'power_balance_{t}')
                    if constr is not None:
                        try:
                            lambda_sample[t] = constr.Pi
                        except:
                            pass
                lambda_vals.append(lambda_sample)
            else:
                lambda_vals.append(np.zeros(self.T))
        
        return np.array(lambda_vals)
    
    def _init_neural_network(self):
        """åˆå§‹åŒ–ä»£ç†çº¦æŸç¥ç»ç½‘ç»œ"""
        input_dim = self.nb * self.T + self.T  # Pd + Î»
        
        self.surrogate_net = SubproblemSurrogateNet(
            input_dim=input_dim,
            T=self.T,
            hidden_dims=[128, 256, 128]
        ).to(self.device)
        
        self.optimizer = optim.Adam(self.surrogate_net.parameters(), lr=1e-4)
        
        print(f"  - ä»£ç†çº¦æŸç½‘ç»œè¾“å…¥ç»´åº¦: {input_dim}", flush=True)
    
    def _initialize_solve(self):
        """åˆå§‹åŒ–æ±‚è§£ï¼Œè·å–åˆå§‹çš„pg, x, coc, cpower"""
        g = self.unit_id
        
        for sample_id in range(self.n_samples):
            lambda_val = self.lambda_vals[sample_id]
            
            model = gp.Model('init_subproblem')
            model.Params.OutputFlag = 0
            
            pg = model.addVars(self.T, lb=0, name='pg')
            x = model.addVars(self.T, vtype=GRB.BINARY, name='x')
            coc = model.addVars(self.T-1, lb=0, name='coc')
            cpower = model.addVars(self.T, lb=0, name='cpower')
            
            # å‘ç”µä¸Šä¸‹é™çº¦æŸ
            for t in range(self.T):
                model.addConstr(pg[t] >= self.gen[g, PMIN] * x[t], name=f'pg_lower_{t}')
                model.addConstr(pg[t] <= self.gen[g, PMAX] * x[t], name=f'pg_upper_{t}')
            
            # çˆ¬å¡çº¦æŸ
            Ru = 0.4 * self.gen[g, PMAX] / self.T_delta
            Rd = 0.4 * self.gen[g, PMAX] / self.T_delta
            for t in range(1, self.T):
                model.addConstr(pg[t] - pg[t-1] <= Ru * x[t-1] + self.gen[g, PMAX] * (1 - x[t-1]), name=f'ramp_up_{t}')
                model.addConstr(pg[t-1] - pg[t] <= Rd * x[t] + self.gen[g, PMAX] * (1 - x[t]), name=f'ramp_down_{t}')
            
            # æœ€å°å¼€å…³æœºæ—¶é—´çº¦æŸ
            Ton = min(4, self.T)
            Toff = min(4, self.T)
            for tau in range(1, Ton+1):
                for t1 in range(self.T - tau):
                    model.addConstr(x[t1+1] - x[t1] <= x[t1+tau], name=f'min_on_{tau}_{t1}')
            for tau in range(1, Toff+1):
                for t1 in range(self.T - tau):
                    model.addConstr(-x[t1+1] + x[t1] <= 1 - x[t1+tau], name=f'min_off_{tau}_{t1}')
            
            # å¯åœæˆæœ¬
            start_cost = self.gencost[g, 1]
            shut_cost = self.gencost[g, 2]
            for t in range(1, self.T):
                model.addConstr(coc[t-1] >= start_cost * (x[t] - x[t-1]), name=f'start_cost_{t}')
                model.addConstr(coc[t-1] >= shut_cost * (x[t-1] - x[t]), name=f'shut_cost_{t}')
            
            # å‘ç”µæˆæœ¬
            for t in range(self.T):
                model.addConstr(cpower[t] >= self.gencost[g, -2]/self.T_delta * pg[t] + 
                              self.gencost[g, -1]/self.T_delta * x[t], name=f'cpower_{t}')
            
            # ç›®æ ‡å‡½æ•°: cost - Î»áµ€ Ã— pg
            obj = gp.quicksum(cpower[t] for t in range(self.T))
            obj += gp.quicksum(coc[t] for t in range(self.T-1))
            obj -= gp.quicksum(lambda_val[t] * pg[t] for t in range(self.T))
            
            model.setObjective(obj, GRB.MINIMIZE)
            model.optimize()
            
            if model.status == GRB.OPTIMAL:
                self.pg[sample_id] = np.array([pg[t].X for t in range(self.T)])
                self.x[sample_id] = np.array([x[t].X for t in range(self.T)])
                self.coc[sample_id] = np.array([coc[t].X for t in range(self.T-1)])
                self.cpower[sample_id] = np.array([cpower[t].X for t in range(self.T)])
    
    def iter_with_primal_block(self, sample_id: int, alpha: np.ndarray, beta: float):
        """
        BCDè¿­ä»£ï¼šåŸå§‹å—
        å›ºå®šä»£ç†çº¦æŸå‚æ•°(alpha, beta)å’Œå¯¹å¶å˜é‡(mu)ï¼Œæ›´æ–°åŸå§‹å˜é‡(pg, x)
        
        ç›®æ ‡å‡½æ•°:
            min  cost - Î»áµ€ Ã— pg 
                 + rho_primal * max(0, sum_t(alpha_t * x_t) - beta)
                 + rho_opt * |sum_t(alpha_t * x_t) - beta| * mu
        """
        g = self.unit_id
        lambda_val = self.lambda_vals[sample_id]
        mu_val = self.mu[sample_id]
        
        model = gp.Model('primal_block')
        model.Params.OutputFlag = 0
        
        # å˜é‡ï¼ˆxä¸ºè¿ç»­ï¼ŒLPæ¾å¼›ï¼‰
        pg = model.addVars(self.T, lb=0, name='pg')
        x = model.addVars(self.T, lb=0, ub=1, name='x')
        coc = model.addVars(self.T-1, lb=0, name='coc')
        cpower = model.addVars(self.T, lb=0, name='cpower')
        
        # ä»£ç†çº¦æŸè¿åé‡
        surrogate_viol = model.addVar(lb=0, name='surrogate_viol')
        surrogate_abs = model.addVar(lb=0, name='surrogate_abs')
        
        # å‘ç”µä¸Šä¸‹é™çº¦æŸ
        for t in range(self.T):
            model.addConstr(pg[t] >= self.gen[g, PMIN] * x[t], name=f'pg_lower_{t}')
            model.addConstr(pg[t] <= self.gen[g, PMAX] * x[t], name=f'pg_upper_{t}')
        
        # çˆ¬å¡çº¦æŸ
        Ru = 0.4 * self.gen[g, PMAX] / self.T_delta
        Rd = 0.4 * self.gen[g, PMAX] / self.T_delta
        for t in range(1, self.T):
            model.addConstr(pg[t] - pg[t-1] <= Ru * x[t-1] + self.gen[g, PMAX] * (1 - x[t-1]), name=f'ramp_up_{t}')
            model.addConstr(pg[t-1] - pg[t] <= Rd * x[t] + self.gen[g, PMAX] * (1 - x[t]), name=f'ramp_down_{t}')
        
        # æœ€å°å¼€å…³æœºæ—¶é—´çº¦æŸ
        Ton = min(4, self.T)
        Toff = min(4, self.T)
        for tau in range(1, Ton+1):
            for t1 in range(self.T - tau):
                model.addConstr(x[t1+1] - x[t1] <= x[t1+tau], name=f'min_on_{tau}_{t1}')
        for tau in range(1, Toff+1):
            for t1 in range(self.T - tau):
                model.addConstr(-x[t1+1] + x[t1] <= 1 - x[t1+tau], name=f'min_off_{tau}_{t1}')
        
        # å¯åœæˆæœ¬
        start_cost = self.gencost[g, 1]
        shut_cost = self.gencost[g, 2]
        for t in range(1, self.T):
            model.addConstr(coc[t-1] >= start_cost * (x[t] - x[t-1]), name=f'start_cost_{t}')
            model.addConstr(coc[t-1] >= shut_cost * (x[t-1] - x[t]), name=f'shut_cost_{t}')
        
        # å‘ç”µæˆæœ¬
        for t in range(self.T):
            model.addConstr(cpower[t] >= self.gencost[g, -2]/self.T_delta * pg[t] + 
                          self.gencost[g, -1]/self.T_delta * x[t], name=f'cpower_{t}')
        
        # ä»£ç†çº¦æŸ: sum_t(alpha_t * x_t) <= beta
        surrogate_lhs = gp.quicksum(alpha[t] * x[t] for t in range(self.T))
        model.addConstr(surrogate_viol >= surrogate_lhs - beta, name='surrogate_viol_constr')
        model.addConstr(surrogate_abs >= surrogate_lhs - beta, name='surrogate_abs_pos')
        model.addConstr(surrogate_abs >= beta - surrogate_lhs, name='surrogate_abs_neg')
        
        # ç›®æ ‡å‡½æ•°
        obj_cost = gp.quicksum(cpower[t] for t in range(self.T))
        obj_cost += gp.quicksum(coc[t] for t in range(self.T-1))
        obj_lambda = -gp.quicksum(lambda_val[t] * pg[t] for t in range(self.T))
        obj_primal = self.rho_primal * surrogate_viol
        obj_opt = self.rho_opt * surrogate_abs * mu_val
        
        model.setObjective(obj_cost + obj_lambda + obj_primal + obj_opt, GRB.MINIMIZE)
        model.optimize()
        
        if model.status == GRB.OPTIMAL:
            pg_sol = np.array([pg[t].X for t in range(self.T)])
            x_sol = np.array([x[t].X for t in range(self.T)])
            coc_sol = np.array([coc[t].X for t in range(self.T-1)])
            cpower_sol = np.array([cpower[t].X for t in range(self.T)])
            return pg_sol, x_sol, coc_sol, cpower_sol
        else:
            print(f"è­¦å‘Š: åŸå§‹å—æ±‚è§£å¤±è´¥ï¼ŒçŠ¶æ€: {model.status}", flush=True)
            return None, None, None, None
    
    def iter_with_dual_block(self, sample_id: int, alpha: np.ndarray, beta: float):
        """
        BCDè¿­ä»£ï¼šå¯¹å¶å—
        å›ºå®šåŸå§‹å˜é‡(pg, x)å’Œä»£ç†çº¦æŸå‚æ•°(alpha, beta)ï¼Œæ›´æ–°å¯¹å¶å˜é‡(mu)
        
        å¯¹å¶é—®é¢˜:
            min  rho_dual * |dual_feasibility| + rho_opt * |constraint_violation| * mu
            s.t. mu >= mu_lower_bound
        """
        g = self.unit_id
        x_val = self.x[sample_id]
        
        model = gp.Model('dual_block')
        model.Params.OutputFlag = 0
        
        # å¯¹å¶å˜é‡
        if self.iter_number < 50:
            mu = model.addVar(lb=self.mu_lower_bound, name='mu')
        else:
            mu = model.addVar(lb=0, name='mu')
        
        # ä»£ç†çº¦æŸè¿åé‡
        surrogate_expr = np.sum(alpha * x_val) - beta
        surrogate_viol = abs(surrogate_expr)
        
        # å¯¹å¶å¯è¡Œæ€§çº¦æŸ
        # å¯¹äºä»£ç†çº¦æŸ sum_t(alpha_t * x_t) <= betaï¼Œ
        # xå˜é‡çš„å¯¹å¶çº¦æŸä¸­éœ€è¦åŠ ä¸Š alpha_t * mu
        # ç®€åŒ–å¤„ç†ï¼šæœ€å°åŒ– mu çš„å¤§å°åŒæ—¶æ»¡è¶³çº¦æŸ
        
        obj_dual = 0
        obj_opt = self.rho_opt * surrogate_viol * mu
        
        model.setObjective(obj_dual + obj_opt, GRB.MINIMIZE)
        model.optimize()
        
        if model.status == GRB.OPTIMAL:
            return mu.X
        else:
            return self.mu_lower_bound
    
    def _extract_features(self, sample_id: int) -> np.ndarray:
        """æå–ç‰¹å¾: [Pd, Î»]"""
        pd_data = self.active_set_data[sample_id]['pd_data']
        pd_flat = pd_data.flatten()
        lambda_val = self.lambda_vals[sample_id]
        return np.concatenate([pd_flat, lambda_val])
    
    def loss_function_differentiable(self, sample_id: int, alpha_tensor: torch.Tensor, 
                                     beta_tensor: torch.Tensor, device) -> torch.Tensor:
        """
        å¯å¾®åˆ†çš„losså‡½æ•°ï¼ˆä¸uc_NN_BCD.pyçš„loss_function_differentiableä¸€è‡´ï¼‰
        
        ä½¿ç”¨BCDè¿­ä»£å¾—åˆ°çš„å˜é‡å€¼(x, mu)è®¡ç®—loss
        
        Loss = rho_primal * obj_primal + rho_dual * obj_dual + rho_opt * obj_opt
        
        å…¶ä¸­:
        - obj_primal: ä»£ç†çº¦æŸè¿åé‡ max(0, sum_t(alpha_t * x_t) - beta)
        - obj_dual: å¯¹å¶çº¦æŸè¿åé‡ |alpha_t * mu - gradient_t|
        - obj_opt: äº’è¡¥æ¾å¼›æ¡ä»¶ |sum_t(alpha_t * x_t) - beta| * mu
        """
        g = self.unit_id
        
        # ä»BCDè¿­ä»£å¾—åˆ°çš„å˜é‡
        x_val = torch.tensor(self.x[sample_id], dtype=torch.float32, device=device)
        mu_val = torch.tensor(self.mu[sample_id], dtype=torch.float32, device=device)
        lambda_val = torch.tensor(self.lambda_vals[sample_id], dtype=torch.float32, device=device)
        
        # æœºç»„å‚æ•°
        gen_PMIN = torch.tensor(self.gen[g, PMIN], dtype=torch.float32, device=device)
        gen_PMAX = torch.tensor(self.gen[g, PMAX], dtype=torch.float32, device=device)
        gencost_var = torch.tensor(self.gencost[g, -2] / self.T_delta, dtype=torch.float32, device=device)
        gencost_fixed = torch.tensor(self.gencost[g, -1] / self.T_delta, dtype=torch.float32, device=device)
        
        # ç›®æ ‡xï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        unit_commitment = self.active_set_data[sample_id].get('unit_commitment_matrix', None)
        if unit_commitment is not None and g < unit_commitment.shape[0]:
            x_target = torch.tensor(unit_commitment[g], dtype=torch.float32, device=device)
        else:
            x_target = None
        
        # ========== è®¡ç®—obj_primal ==========
        # ä»£ç†çº¦æŸè¿åé‡: max(0, sum_t(alpha_t * x_t) - beta)
        surrogate_lhs = torch.sum(alpha_tensor * x_val)
        surrogate_violation = torch.relu(surrogate_lhs - beta_tensor.squeeze())
        obj_primal = surrogate_violation
        
        # ========== è®¡ç®—obj_opt ==========
        # äº’è¡¥æ¾å¼›: |sum_t(alpha_t * x_t) - beta| * mu
        surrogate_abs = torch.abs(surrogate_lhs - beta_tensor.squeeze())
        obj_opt = surrogate_abs * mu_val
        
        # ========== è®¡ç®—obj_dual ==========
        # xå˜é‡çš„å¯¹å¶çº¦æŸ: gencost_fixed - lambda + ... + alpha * mu = 0
        # ç®€åŒ–ç‰ˆæœ¬ï¼šåªè€ƒè™‘ä»£ç†çº¦æŸçš„è´¡çŒ®
        obj_dual = torch.tensor(0.0, device=device)
        for t in range(self.T):
            # åŸºç¡€å¯¹å¶è¡¨è¾¾å¼ï¼ˆç®€åŒ–ï¼‰
            dual_expr = gencost_fixed - lambda_val[t]
            # ä»£ç†çº¦æŸçš„å¯¹å¶è´¡çŒ®
            dual_expr = dual_expr + alpha_tensor[t] * mu_val
            obj_dual = obj_dual + torch.abs(dual_expr)
        
        # ========== é™„åŠ æŸå¤±ï¼šç¡®ä¿ä»£ç†çº¦æŸæœ‰æ•ˆ ==========
        # 1. ä»£ç†çº¦æŸåº”è¯¥åˆ‡æ‰LPæ¾å¼›è§£ä½†ä¿ç•™æ•´æ•°è§£
        if x_target is not None:
            # å¯¹äºçœŸå®æ•´æ•°è§£ï¼Œä»£ç†çº¦æŸåº”è¯¥è¢«æ»¡è¶³
            target_lhs = torch.sum(alpha_tensor * x_target)
            target_violation = torch.relu(target_lhs - beta_tensor.squeeze())
            loss_target_feasibility = target_violation * 10.0  # å¤§æƒé‡
        else:
            loss_target_feasibility = torch.tensor(0.0, device=device)
        
        # 2. æ•´æ•°æ€§æŸå¤±ï¼šé¼“åŠ±xæ¥è¿‘0æˆ–1
        loss_integrality = torch.sum(x_val * (1 - x_val))
        
        # 3. ä¸ç›®æ ‡xçš„åå·®
        if x_target is not None:
            loss_deviation = torch.sum((x_val - x_target) ** 2)
        else:
            loss_deviation = torch.tensor(0.0, device=device)
        
        # æ€»æŸå¤±
        loss = (self.rho_primal * obj_primal + 
                self.rho_dual * obj_dual + 
                self.rho_opt * obj_opt +
                loss_target_feasibility +
                0.1 * loss_integrality +
                loss_deviation)
        
        return loss
    
    def iter_with_surrogate_nn(self, num_epochs: int = 10):
        """
        BCDè¿­ä»£ï¼šç¥ç»ç½‘ç»œæ›´æ–°ä»£ç†çº¦æŸå‚æ•°
        ä½¿ç”¨loss_function_differentiableè¿›è¡Œè®­ç»ƒ
        """
        if not TORCH_AVAILABLE:
            return
        
        self.surrogate_net.train()
        
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            
            for sample_id in range(self.n_samples):
                # æå–ç‰¹å¾
                features = self._extract_features(sample_id)
                features_tensor = torch.tensor(features, dtype=torch.float32, device=self.device).unsqueeze(0)
                
                # å‰å‘ä¼ æ’­
                alpha_out, beta_out = self.surrogate_net(features_tensor)
                alpha_tensor = alpha_out.squeeze(0)
                beta_tensor = beta_out.squeeze(0)
                
                # è®¡ç®—loss
                self.optimizer.zero_grad()
                loss = self.loss_function_differentiable(
                    sample_id, alpha_tensor, beta_tensor, self.device
                )
                
                # åå‘ä¼ æ’­
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.surrogate_net.parameters(), max_norm=1.0)
                self.optimizer.step()
                
                epoch_loss += loss.item()
                
                # æ›´æ–°alphaå’Œbetaå€¼
                self.alpha_values[sample_id] = alpha_tensor.detach().cpu().numpy()
                self.beta_values[sample_id] = beta_tensor.detach().cpu().numpy().item()
            
            if epoch == 0 or epoch == num_epochs - 1:
                print(f"  [NN] epoch {epoch+1}/{num_epochs}, avg_loss = {epoch_loss/self.n_samples:.6f}", flush=True)
    
    def cal_viol(self) -> Tuple[float, float, float]:
        """è®¡ç®—çº¦æŸè¿åé‡"""
        obj_primal = 0.0
        obj_dual = 0.0
        obj_opt = 0.0
        
        g = self.unit_id
        
        for sample_id in range(self.n_samples):
            x_val = self.x[sample_id]
            alpha = self.alpha_values[sample_id]
            beta = self.beta_values[sample_id]
            mu_val = self.mu[sample_id]
            lambda_val = self.lambda_vals[sample_id]
            
            # ä»£ç†çº¦æŸè¿å
            surrogate_lhs = np.sum(alpha * x_val)
            surrogate_viol = max(0, surrogate_lhs - beta)
            obj_primal += surrogate_viol
            
            # äº’è¡¥æ¾å¼›
            obj_opt += abs(surrogate_lhs - beta) * mu_val
            
            # å¯¹å¶çº¦æŸï¼ˆç®€åŒ–ï¼‰
            gencost_fixed = self.gencost[g, -1] / self.T_delta
            for t in range(self.T):
                dual_expr = gencost_fixed - lambda_val[t] + alpha[t] * mu_val
                obj_dual += abs(dual_expr)
        
        return obj_primal, obj_dual, obj_opt
    
    def iter(self, max_iter: int = 20, nn_epochs: int = 10):
        """
        ä¸»BCDè¿­ä»£å¾ªç¯
        """
        print(f"å¼€å§‹BCDè¿­ä»£è®­ç»ƒ (æœºç»„{self.unit_id})...", flush=True)
        
        for i in range(max_iter):
            print(f"ğŸ”„ è¿­ä»£ {i+1}/{max_iter}", flush=True)
            self.iter_number = i
            
            EPS = 1e-10
            
            # 1. åŸå§‹å—è¿­ä»£
            for sample_id in range(self.n_samples):
                alpha = self.alpha_values[sample_id]
                beta = self.beta_values[sample_id]
                
                pg_sol, x_sol, coc_sol, cpower_sol = self.iter_with_primal_block(
                    sample_id, alpha, beta
                )
                
                if pg_sol is not None:
                    self.pg[sample_id] = np.where(np.abs(pg_sol) < EPS, 0, pg_sol)
                    self.x[sample_id] = np.where(np.abs(x_sol) < EPS, 0, x_sol)
                    self.x[sample_id] = np.where(np.abs(self.x[sample_id] - 1) < EPS, 1, self.x[sample_id])
                    self.coc[sample_id] = np.where(np.abs(coc_sol) < EPS, 0, coc_sol)
                    self.cpower[sample_id] = np.where(np.abs(cpower_sol) < EPS, 0, cpower_sol)
            
            # 2. å¯¹å¶å—è¿­ä»£
            for sample_id in range(self.n_samples):
                alpha = self.alpha_values[sample_id]
                beta = self.beta_values[sample_id]
                
                mu_sol = self.iter_with_dual_block(sample_id, alpha, beta)
                self.mu[sample_id] = max(mu_sol, 0) if self.iter_number >= 50 else max(mu_sol, self.mu_lower_bound)
            
            # 3. ç¥ç»ç½‘ç»œæ›´æ–°ä»£ç†çº¦æŸå‚æ•°
            self.iter_with_surrogate_nn(num_epochs=nn_epochs)
            
            # è®¡ç®—è¿åé‡
            obj_primal, obj_dual, obj_opt = self.cal_viol()
            obj_primal = obj_primal if abs(obj_primal) >= 1e-12 else 0.0
            obj_dual = obj_dual if abs(obj_dual) >= 1e-12 else 0.0
            obj_opt = obj_opt if abs(obj_opt) >= 1e-12 else 0.0
            
            print(f"  obj_primal: {obj_primal:.6f}, obj_dual: {obj_dual:.6f}, obj_opt: {obj_opt:.6f}", flush=True)
            
            # æ›´æ–°æƒ©ç½šå‚æ•°
            self.rho_primal += self.gamma * obj_primal
            self.rho_dual += self.gamma * obj_dual
            self.rho_opt += self.gamma * obj_opt
            
            print(f"  Ï_primal={self.rho_primal:.4f}, Ï_dual={self.rho_dual:.4f}, Ï_opt={self.rho_opt:.4f}", flush=True)
            print("  " + "-" * 40, flush=True)
        
        print(f"âœ“ æœºç»„{self.unit_id}ä»£ç†çº¦æŸè®­ç»ƒå®Œæˆ", flush=True)
    
    def get_surrogate_params(self, pd_data: np.ndarray, lambda_val: np.ndarray) -> Tuple[np.ndarray, float]:
        """è·å–ä»£ç†çº¦æŸå‚æ•°"""
        if not TORCH_AVAILABLE:
            raise RuntimeError("PyTorchä¸å¯ç”¨")
        
        self.surrogate_net.eval()
        
        pd_flat = pd_data.flatten()
        features = np.concatenate([pd_flat, lambda_val])
        features_tensor = torch.tensor(features, dtype=torch.float32, device=self.device).unsqueeze(0)
        
        with torch.no_grad():
            alpha, beta = self.surrogate_net(features_tensor)
        
        return alpha.squeeze(0).cpu().numpy(), beta.squeeze(0).cpu().numpy().item()
    
    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        if TORCH_AVAILABLE:
            state = {
                'surrogate_net_state_dict': self.surrogate_net.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'alpha_values': self.alpha_values,
                'beta_values': self.beta_values,
                'mu': self.mu,
                'rho_primal': self.rho_primal,
                'rho_dual': self.rho_dual,
                'rho_opt': self.rho_opt
            }
            
            dirpath = os.path.dirname(os.path.abspath(filepath))
            if dirpath and not os.path.exists(dirpath):
                os.makedirs(dirpath, exist_ok=True)
            
            torch.save(state, filepath)
            print(f"âœ“ ä»£ç†çº¦æŸæ¨¡å‹å·²ä¿å­˜: {filepath}", flush=True)
    
    def load(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        if TORCH_AVAILABLE:
            state = torch.load(filepath, map_location=self.device)
            self.surrogate_net.load_state_dict(state['surrogate_net_state_dict'])
            self.optimizer.load_state_dict(state['optimizer_state_dict'])
            self.alpha_values = state['alpha_values']
            self.beta_values = state['beta_values']
            self.mu = state['mu']
            self.rho_primal = state['rho_primal']
            self.rho_dual = state['rho_dual']
            self.rho_opt = state['rho_opt']
            print(f"âœ“ ä»£ç†çº¦æŸæ¨¡å‹å·²åŠ è½½: {filepath}", flush=True)


# ========================== è®­ç»ƒä»£ç  ==========================

def train_dual_predictor_from_data(ppc, active_set_data: List[Dict], T_delta: float = 1.0,
                                    num_epochs: int = 100, batch_size: int = 8,
                                    save_path: str = None, device=None) -> DualVariablePredictorTrainer:
    """
    è®­ç»ƒå¯¹å¶å˜é‡é¢„æµ‹å™¨
    
    Args:
        ppc: PyPoweræ¡ˆä¾‹æ•°æ®
        active_set_data: æ´»åŠ¨é›†æ•°æ®åˆ—è¡¨
        T_delta: æ—¶é—´é—´éš”
        num_epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        save_path: æ¨¡å‹ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        è®­ç»ƒå¥½çš„å¯¹å¶å˜é‡é¢„æµ‹å™¨
    """
    print("=" * 60, flush=True)
    print("è®­ç»ƒå¯¹å¶å˜é‡é¢„æµ‹å™¨", flush=True)
    print("=" * 60, flush=True)
    
    # åˆ›å»ºé¢„æµ‹å™¨
    predictor = DualVariablePredictorTrainer(ppc, active_set_data, T_delta, device)
    
    # è®­ç»ƒ
    predictor.train(num_epochs=num_epochs, batch_size=batch_size)
    
    # ä¿å­˜æ¨¡å‹
    if save_path:
        predictor.save(save_path)
    
    return predictor


def train_subproblem_surrogate_from_data(ppc, active_set_data: List[Dict], unit_id: int,
                                          T_delta: float = 1.0, lambda_predictor=None,
                                          max_iter: int = 20, nn_epochs: int = 10,
                                          save_path: str = None, device=None) -> SubproblemSurrogateTrainer:
    """
    è®­ç»ƒå•æœºç»„å­é—®é¢˜ä»£ç†çº¦æŸ
    
    Args:
        ppc: PyPoweræ¡ˆä¾‹æ•°æ®
        active_set_data: æ´»åŠ¨é›†æ•°æ®åˆ—è¡¨
        unit_id: æœºç»„ID
        T_delta: æ—¶é—´é—´éš”
        lambda_predictor: å·²è®­ç»ƒçš„å¯¹å¶å˜é‡é¢„æµ‹å™¨ï¼ˆå¯é€‰ï¼‰
        max_iter: BCDæœ€å¤§è¿­ä»£æ¬¡æ•°
        nn_epochs: æ¯æ¬¡BCDè¿­ä»£ä¸­ç¥ç»ç½‘ç»œè®­ç»ƒè½®æ•°
        save_path: æ¨¡å‹ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        è®­ç»ƒå¥½çš„ä»£ç†çº¦æŸè®­ç»ƒå™¨
    """
    print("=" * 60, flush=True)
    print(f"è®­ç»ƒæœºç»„{unit_id}å­é—®é¢˜ä»£ç†çº¦æŸ", flush=True)
    print("=" * 60, flush=True)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SubproblemSurrogateTrainer(
        ppc, active_set_data, T_delta, unit_id,
        lambda_predictor=lambda_predictor, device=device
    )
    
    # BCDè¿­ä»£è®­ç»ƒ
    trainer.iter(max_iter=max_iter, nn_epochs=nn_epochs)
    
    # ä¿å­˜æ¨¡å‹
    if save_path:
        trainer.save(save_path)
    
    return trainer


def train_all_subproblem_surrogates(ppc, active_set_data: List[Dict], T_delta: float = 1.0,
                                     lambda_predictor=None, unit_ids: List[int] = None,
                                     max_iter: int = 20, nn_epochs: int = 10,
                                     save_dir: str = None, device=None) -> Dict[int, SubproblemSurrogateTrainer]:
    """
    è®­ç»ƒæ‰€æœ‰æœºç»„çš„å­é—®é¢˜ä»£ç†çº¦æŸ
    
    Args:
        ppc: PyPoweræ¡ˆä¾‹æ•°æ®
        active_set_data: æ´»åŠ¨é›†æ•°æ®åˆ—è¡¨
        T_delta: æ—¶é—´é—´éš”
        lambda_predictor: å·²è®­ç»ƒçš„å¯¹å¶å˜é‡é¢„æµ‹å™¨ï¼ˆå¯é€‰ï¼‰
        unit_ids: è¦è®­ç»ƒçš„æœºç»„IDåˆ—è¡¨ï¼ˆé»˜è®¤ä¸ºæ‰€æœ‰æœºç»„ï¼‰
        max_iter: BCDæœ€å¤§è¿­ä»£æ¬¡æ•°
        nn_epochs: æ¯æ¬¡BCDè¿­ä»£ä¸­ç¥ç»ç½‘ç»œè®­ç»ƒè½®æ•°
        save_dir: æ¨¡å‹ä¿å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        æ‰€æœ‰æœºç»„çš„ä»£ç†çº¦æŸè®­ç»ƒå™¨å­—å…¸ {unit_id: trainer}
    """
    ppc_int = ext2int(ppc)
    ng = ppc_int['gen'].shape[0]
    
    if unit_ids is None:
        unit_ids = list(range(ng))
    
    print("=" * 60, flush=True)
    print(f"è®­ç»ƒæ‰€æœ‰æœºç»„ä»£ç†çº¦æŸ ({len(unit_ids)} ä¸ªæœºç»„)", flush=True)
    print("=" * 60, flush=True)
    
    trainers = {}
    
    for i, g in enumerate(unit_ids):
        print(f"\n>>> æœºç»„ {g} ({i+1}/{len(unit_ids)}) <<<", flush=True)
        
        trainer = SubproblemSurrogateTrainer(
            ppc, active_set_data, T_delta, g,
            lambda_predictor=lambda_predictor, device=device
        )
        
        trainer.iter(max_iter=max_iter, nn_epochs=nn_epochs)
        trainers[g] = trainer
        
        if save_dir:
            os.makedirs(save_dir, exist_ok=True)
            trainer.save(os.path.join(save_dir, f'surrogate_unit_{g}.pth'))
    
    print(f"\nâœ“ æ‰€æœ‰æœºç»„ä»£ç†çº¦æŸè®­ç»ƒå®Œæˆ", flush=True)
    return trainers


def train_complete_model(ppc, active_set_data: List[Dict], T_delta: float = 1.0,
                          unit_ids: List[int] = None,
                          dual_epochs: int = 100, dual_batch_size: int = 8,
                          surrogate_max_iter: int = 20, surrogate_nn_epochs: int = 10,
                          save_dir: str = None, device=None):
    """
    å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼šå…ˆè®­ç»ƒå¯¹å¶é¢„æµ‹å™¨ï¼Œå†è®­ç»ƒæ‰€æœ‰æœºç»„çš„ä»£ç†çº¦æŸ
    
    Args:
        ppc: PyPoweræ¡ˆä¾‹æ•°æ®
        active_set_data: æ´»åŠ¨é›†æ•°æ®åˆ—è¡¨
        T_delta: æ—¶é—´é—´éš”
        unit_ids: è¦è®­ç»ƒçš„æœºç»„IDåˆ—è¡¨ï¼ˆé»˜è®¤ä¸ºæ‰€æœ‰æœºç»„ï¼‰
        dual_epochs: å¯¹å¶é¢„æµ‹å™¨è®­ç»ƒè½®æ•°
        dual_batch_size: å¯¹å¶é¢„æµ‹å™¨æ‰¹æ¬¡å¤§å°
        surrogate_max_iter: ä»£ç†çº¦æŸBCDæœ€å¤§è¿­ä»£æ¬¡æ•°
        surrogate_nn_epochs: ä»£ç†çº¦æŸç¥ç»ç½‘ç»œè®­ç»ƒè½®æ•°
        save_dir: æ¨¡å‹ä¿å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        (dual_predictor, trainers) å…ƒç»„
    """
    print("\n" + "=" * 60, flush=True)
    print("å¼€å§‹å®Œæ•´æ¨¡å‹è®­ç»ƒ", flush=True)
    print("=" * 60, flush=True)
    
    ppc_int = ext2int(ppc)
    ng = ppc_int['gen'].shape[0]
    n_samples = len(active_set_data)
    
    if unit_ids is None:
        unit_ids = list(range(ng))
    
    print(f"\né…ç½®ä¿¡æ¯:", flush=True)
    print(f"  - æ ·æœ¬æ•°é‡: {n_samples}", flush=True)
    print(f"  - æœºç»„æ•°é‡: {ng} (è®­ç»ƒ{len(unit_ids)}ä¸ª)", flush=True)
    print(f"  - å¯¹å¶é¢„æµ‹å™¨è®­ç»ƒè½®æ•°: {dual_epochs}", flush=True)
    print(f"  - ä»£ç†çº¦æŸBCDè¿­ä»£æ¬¡æ•°: {surrogate_max_iter}", flush=True)
    print(f"  - ä»£ç†çº¦æŸNNè®­ç»ƒè½®æ•°/è¿­ä»£: {surrogate_nn_epochs}", flush=True)
    
    # æ­¥éª¤1: è®­ç»ƒå¯¹å¶å˜é‡é¢„æµ‹å™¨
    print("\n" + "-" * 40, flush=True)
    print("ã€æ­¥éª¤1ã€‘è®­ç»ƒå¯¹å¶å˜é‡é¢„æµ‹å™¨", flush=True)
    print("-" * 40, flush=True)
    
    dual_save_path = os.path.join(save_dir, 'dual_predictor.pth') if save_dir else None
    dual_predictor = train_dual_predictor_from_data(
        ppc, active_set_data, T_delta,
        num_epochs=dual_epochs, batch_size=dual_batch_size,
        save_path=dual_save_path, device=device
    )
    
    # æ­¥éª¤2: è®­ç»ƒæ‰€æœ‰æœºç»„çš„ä»£ç†çº¦æŸ
    print("\n" + "-" * 40, flush=True)
    print("ã€æ­¥éª¤2ã€‘è®­ç»ƒæœºç»„ä»£ç†çº¦æŸ", flush=True)
    print("-" * 40, flush=True)
    
    trainers = train_all_subproblem_surrogates(
        ppc, active_set_data, T_delta,
        lambda_predictor=dual_predictor, unit_ids=unit_ids,
        max_iter=surrogate_max_iter, nn_epochs=surrogate_nn_epochs,
        save_dir=save_dir, device=device
    )
    
    print("\n" + "=" * 60, flush=True)
    print("å®Œæ•´æ¨¡å‹è®­ç»ƒå®Œæˆ!", flush=True)
    print("=" * 60, flush=True)
    
    return dual_predictor, trainers


def load_trained_models(ppc, active_set_data: List[Dict], T_delta: float,
                        load_dir: str, unit_ids: List[int] = None, device=None):
    """
    åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹
    
    Args:
        ppc: PyPoweræ¡ˆä¾‹æ•°æ®
        active_set_data: æ´»åŠ¨é›†æ•°æ®åˆ—è¡¨
        T_delta: æ—¶é—´é—´éš”
        load_dir: æ¨¡å‹åŠ è½½ç›®å½•
        unit_ids: è¦åŠ è½½çš„æœºç»„IDåˆ—è¡¨
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        (dual_predictor, trainers) å…ƒç»„
    """
    ppc_int = ext2int(ppc)
    ng = ppc_int['gen'].shape[0]
    
    if unit_ids is None:
        unit_ids = list(range(ng))
    
    print(f"ä» {load_dir} åŠ è½½æ¨¡å‹...", flush=True)
    
    # åŠ è½½å¯¹å¶é¢„æµ‹å™¨
    dual_predictor = DualVariablePredictorTrainer(ppc, active_set_data, T_delta, device)
    dual_path = os.path.join(load_dir, 'dual_predictor.pth')
    if os.path.exists(dual_path):
        dual_predictor.load(dual_path)
    else:
        print(f"è­¦å‘Š: æœªæ‰¾åˆ°å¯¹å¶é¢„æµ‹å™¨æ¨¡å‹ {dual_path}", flush=True)
    
    # åŠ è½½ä»£ç†çº¦æŸæ¨¡å‹
    trainers = {}
    for g in unit_ids:
        trainer = SubproblemSurrogateTrainer(
            ppc, active_set_data, T_delta, g,
            lambda_predictor=dual_predictor, device=device
        )
        
        surrogate_path = os.path.join(load_dir, f'surrogate_unit_{g}.pth')
        if os.path.exists(surrogate_path):
            trainer.load(surrogate_path)
            trainers[g] = trainer
        else:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ°æœºç»„{g}ä»£ç†çº¦æŸæ¨¡å‹ {surrogate_path}", flush=True)
    
    print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ", flush=True)
    return dual_predictor, trainers


def evaluate_trained_models(dual_predictor: DualVariablePredictorTrainer,
                            trainers: Dict[int, SubproblemSurrogateTrainer],
                            active_set_data: List[Dict], n_eval_samples: int = 5):
    """
    è¯„ä¼°å·²è®­ç»ƒæ¨¡å‹çš„æ•ˆæœ
    
    Args:
        dual_predictor: å¯¹å¶å˜é‡é¢„æµ‹å™¨
        trainers: ä»£ç†çº¦æŸè®­ç»ƒå™¨å­—å…¸
        active_set_data: æ´»åŠ¨é›†æ•°æ®
        n_eval_samples: è¯„ä¼°æ ·æœ¬æ•°é‡
    """
    print("\n" + "=" * 60, flush=True)
    print("æ¨¡å‹è¯„ä¼°", flush=True)
    print("=" * 60, flush=True)
    
    n_eval = min(n_eval_samples, len(active_set_data))
    
    # 1. è¯„ä¼°å¯¹å¶é¢„æµ‹å™¨
    print("\n--- å¯¹å¶å˜é‡é¢„æµ‹å™¨è¯„ä¼° ---", flush=True)
    total_mse = 0.0
    total_mae = 0.0
    
    for sample_id in range(n_eval):
        pd_data = active_set_data[sample_id]['pd_data']
        lambda_pred = dual_predictor.predict(pd_data)
        lambda_true = dual_predictor.lambda_true[sample_id]
        
        mse = np.mean((lambda_pred - lambda_true) ** 2)
        mae = np.mean(np.abs(lambda_pred - lambda_true))
        total_mse += mse
        total_mae += mae
    
    print(f"  å¹³å‡MSE: {total_mse / n_eval:.6f}", flush=True)
    print(f"  å¹³å‡MAE: {total_mae / n_eval:.6f}", flush=True)
    
    # 2. è¯„ä¼°ä»£ç†çº¦æŸ
    print("\n--- ä»£ç†çº¦æŸè¯„ä¼° ---", flush=True)
    
    for g, trainer in trainers.items():
        total_gap_without = 0.0
        total_gap_with = 0.0
        feasible_count = 0
        
        for sample_id in range(n_eval):
            lambda_val = trainer.lambda_vals[sample_id]
            alpha = trainer.alpha_values[sample_id]
            beta = trainer.beta_values[sample_id]
            
            # æ— ä»£ç†çº¦æŸ
            x_without = solve_subproblem_LP_simple(trainer, sample_id, lambda_val, None, None)
            gap_without = np.sum(x_without * (1 - x_without))
            total_gap_without += gap_without
            
            # æœ‰ä»£ç†çº¦æŸ
            x_with = solve_subproblem_LP_simple(trainer, sample_id, lambda_val, alpha, beta)
            gap_with = np.sum(x_with * (1 - x_with))
            total_gap_with += gap_with
            
            # çœŸå®è§£å¯è¡Œæ€§
            unit_commitment = active_set_data[sample_id].get('unit_commitment_matrix', None)
            if unit_commitment is not None and g < unit_commitment.shape[0]:
                x_target = unit_commitment[g]
                if np.sum(alpha * x_target) <= beta + 1e-6:
                    feasible_count += 1
        
        avg_gap_without = total_gap_without / n_eval
        avg_gap_with = total_gap_with / n_eval
        gap_reduction = (avg_gap_without - avg_gap_with) / max(avg_gap_without, 1e-6) * 100
        feasibility_rate = feasible_count / n_eval * 100
        
        print(f"\n  æœºç»„ {g}:", flush=True)
        print(f"    æ•´æ•°æ€§é—´éš™ (æ— ä»£ç†): {avg_gap_without:.4f}", flush=True)
        print(f"    æ•´æ•°æ€§é—´éš™ (æœ‰ä»£ç†): {avg_gap_with:.4f}", flush=True)
        print(f"    é—´éš™å‡å°‘: {gap_reduction:.2f}%", flush=True)
        print(f"    çœŸå®è§£å¯è¡Œç‡: {feasibility_rate:.1f}%", flush=True)


def train_from_json_file(json_filepath: str, ppc, T_delta: float = 1.0,
                          unit_ids: List[int] = None, save_dir: str = None,
                          dual_epochs: int = 100, surrogate_max_iter: int = 20,
                          surrogate_nn_epochs: int = 10, device=None):
    """
    ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®å¹¶è®­ç»ƒæ¨¡å‹
    
    Args:
        json_filepath: JSONæ•°æ®æ–‡ä»¶è·¯å¾„
        ppc: PyPoweræ¡ˆä¾‹æ•°æ®
        T_delta: æ—¶é—´é—´éš”
        unit_ids: è¦è®­ç»ƒçš„æœºç»„IDåˆ—è¡¨
        save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        dual_epochs: å¯¹å¶é¢„æµ‹å™¨è®­ç»ƒè½®æ•°
        surrogate_max_iter: ä»£ç†çº¦æŸBCDè¿­ä»£æ¬¡æ•°
        surrogate_nn_epochs: ä»£ç†çº¦æŸNNè®­ç»ƒè½®æ•°
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        (dual_predictor, trainers) å…ƒç»„
    """
    print(f"ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®: {json_filepath}", flush=True)
    
    # åŠ è½½æ•°æ®
    active_set_data = load_active_set_from_json(json_filepath)
    print(f"åŠ è½½äº† {len(active_set_data)} ä¸ªæ ·æœ¬", flush=True)
    
    # è®­ç»ƒæ¨¡å‹
    dual_predictor, trainers = train_complete_model(
        ppc, active_set_data, T_delta,
        unit_ids=unit_ids,
        dual_epochs=dual_epochs,
        surrogate_max_iter=surrogate_max_iter,
        surrogate_nn_epochs=surrogate_nn_epochs,
        save_dir=save_dir,
        device=device
    )
    
    # è¯„ä¼°æ¨¡å‹
    evaluate_trained_models(dual_predictor, trainers, active_set_data)
    
    return dual_predictor, trainers


# ========================== æµ‹è¯•ä»£ç  ==========================

def generate_test_data(ppc, T: int = 8, n_samples: int = 10, seed: int = 42) -> List[Dict]:
    """
    ç”Ÿæˆæµ‹è¯•ç”¨çš„æ´»åŠ¨é›†æ•°æ®
    
    Args:
        ppc: PyPoweræ¡ˆä¾‹æ•°æ®
        T: æ—¶æ®µæ•°
        n_samples: æ ·æœ¬æ•°é‡
        seed: éšæœºç§å­
        
    Returns:
        æ´»åŠ¨é›†æ•°æ®åˆ—è¡¨
    """
    ppc_int = ext2int(ppc)
    nb = ppc_int['bus'].shape[0]
    ng = ppc_int['gen'].shape[0]
    
    active_set_data = []
    
    for sample_id in range(n_samples):
        np.random.seed(seed + sample_id)
        
        # ç”Ÿæˆéšæœºè´Ÿè·æ•°æ®ï¼ˆå¸¦æœ‰æ—¥å˜åŒ–æ›²çº¿ï¼‰
        base_load = np.random.uniform(50, 150, nb)
        time_factor = 1 + 0.3 * np.sin(np.linspace(0, 2*np.pi, T)) + 0.1 * np.random.randn(T)
        pd_data = np.outer(base_load, time_factor)
        pd_data = np.maximum(pd_data, 10)  # ç¡®ä¿è´Ÿè·ä¸ºæ­£
        
        # ç”Ÿæˆéšæœºçš„æœºç»„å¯åœçŠ¶æ€ï¼ˆæ»¡è¶³éƒ¨åˆ†çº¦æŸï¼‰
        unit_commitment = np.zeros((ng, T), dtype=int)
        for g in range(ng):
            # éšæœºé€‰æ‹©å¼€æœºæ—¶æ®µ
            on_probability = 0.6 + 0.3 * np.random.rand()
            for t in range(T):
                if np.random.rand() < on_probability:
                    unit_commitment[g, t] = 1
        
        sample = {
            'sample_id': sample_id,
            'pd_data': pd_data,
            'unit_commitment_matrix': unit_commitment,
            'active_constraints': [],
            'active_variables': []
        }
        active_set_data.append(sample)
    
    print(f"âœ“ ç”Ÿæˆäº† {n_samples} ä¸ªæµ‹è¯•æ ·æœ¬ (T={T}, nb={nb}, ng={ng})", flush=True)
    return active_set_data


def test_dual_predictor(ppc=None, active_set_data=None, save_path: str = None):
    """
    æµ‹è¯•å¯¹å¶å˜é‡é¢„æµ‹å™¨
    
    Args:
        ppc: PyPoweræ¡ˆä¾‹æ•°æ®ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨case30ï¼‰
        active_set_data: æ´»åŠ¨é›†æ•°æ®ï¼ˆå¦‚æœä¸ºNoneåˆ™ç”Ÿæˆï¼‰
        save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        
    Returns:
        è®­ç»ƒå¥½çš„é¢„æµ‹å™¨
    """
    if not PYPOWER_AVAILABLE:
        print("pypoweræœªå®‰è£…ï¼Œè·³è¿‡æµ‹è¯•", flush=True)
        return None
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•1: å¯¹å¶å˜é‡é¢„æµ‹å™¨è®­ç»ƒ")
    print("=" * 60)
    
    # å‡†å¤‡æ•°æ®
    if ppc is None:
        ppc = pypower.case30.case30()
    
    T = 8
    if active_set_data is None:
        active_set_data = generate_test_data(ppc, T=T, n_samples=20)
    
    # åˆ›å»ºå¹¶è®­ç»ƒé¢„æµ‹å™¨
    print("\n--- åˆå§‹åŒ–é¢„æµ‹å™¨ ---", flush=True)
    predictor = DualVariablePredictorTrainer(ppc, active_set_data, T_delta=1.0)
    
    print("\n--- å¼€å§‹è®­ç»ƒ ---", flush=True)
    predictor.train(num_epochs=100, batch_size=8)
    
    # è¯„ä¼°é¢„æµ‹æ•ˆæœ
    print("\n--- è¯„ä¼°é¢„æµ‹æ•ˆæœ ---", flush=True)
    total_mse = 0.0
    total_mae = 0.0
    
    for sample_id in range(min(5, len(active_set_data))):
        test_pd = active_set_data[sample_id]['pd_data']
        lambda_pred = predictor.predict(test_pd)
        lambda_true = predictor.lambda_true[sample_id]
        
        mse = np.mean((lambda_pred - lambda_true) ** 2)
        mae = np.mean(np.abs(lambda_pred - lambda_true))
        total_mse += mse
        total_mae += mae
        
        if sample_id < 3:
            print(f"\n  æ ·æœ¬ {sample_id}:", flush=True)
            print(f"    é¢„æµ‹: {lambda_pred[:4]}... (å‰4ä¸ªæ—¶æ®µ)", flush=True)
            print(f"    çœŸå€¼: {lambda_true[:4]}...", flush=True)
            print(f"    MSE: {mse:.6f}, MAE: {mae:.6f}", flush=True)
    
    avg_mse = total_mse / min(5, len(active_set_data))
    avg_mae = total_mae / min(5, len(active_set_data))
    print(f"\n  å¹³å‡MSE: {avg_mse:.6f}", flush=True)
    print(f"  å¹³å‡MAE: {avg_mae:.6f}", flush=True)
    
    # ä¿å­˜æ¨¡å‹
    if save_path:
        predictor.save(save_path)
    
    print("\nâœ“ å¯¹å¶å˜é‡é¢„æµ‹å™¨æµ‹è¯•å®Œæˆ", flush=True)
    return predictor


def test_subproblem_surrogate(ppc=None, active_set_data=None, lambda_predictor=None,
                              unit_id: int = 0, save_path: str = None):
    """
    æµ‹è¯•å­é—®é¢˜ä»£ç†çº¦æŸè®­ç»ƒ
    
    Args:
        ppc: PyPoweræ¡ˆä¾‹æ•°æ®
        active_set_data: æ´»åŠ¨é›†æ•°æ®
        lambda_predictor: å·²è®­ç»ƒçš„å¯¹å¶å˜é‡é¢„æµ‹å™¨
        unit_id: æµ‹è¯•çš„æœºç»„ID
        save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        
    Returns:
        è®­ç»ƒå¥½çš„ä»£ç†çº¦æŸè®­ç»ƒå™¨
    """
    if not PYPOWER_AVAILABLE:
        print("pypoweræœªå®‰è£…ï¼Œè·³è¿‡æµ‹è¯•", flush=True)
        return None
    
    print("\n" + "=" * 60)
    print(f"æµ‹è¯•2: æœºç»„{unit_id}å­é—®é¢˜ä»£ç†çº¦æŸè®­ç»ƒ (BCDæ–¹å¼)")
    print("=" * 60)
    
    # å‡†å¤‡æ•°æ®
    if ppc is None:
        ppc = pypower.case30.case30()
    
    T = 8
    if active_set_data is None:
        active_set_data = generate_test_data(ppc, T=T, n_samples=15)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("\n--- åˆå§‹åŒ–ä»£ç†çº¦æŸè®­ç»ƒå™¨ ---", flush=True)
    trainer = SubproblemSurrogateTrainer(
        ppc, active_set_data, T_delta=1.0, unit_id=unit_id,
        lambda_predictor=lambda_predictor
    )
    
    # BCDè¿­ä»£è®­ç»ƒ
    print("\n--- å¼€å§‹BCDè¿­ä»£è®­ç»ƒ ---", flush=True)
    trainer.iter(max_iter=15, nn_epochs=8)
    
    # è¯„ä¼°ä»£ç†çº¦æŸæ•ˆæœ
    print("\n--- è¯„ä¼°ä»£ç†çº¦æŸæ•ˆæœ ---", flush=True)
    evaluate_surrogate_effectiveness(trainer, active_set_data)
    
    # ä¿å­˜æ¨¡å‹
    if save_path:
        trainer.save(save_path)
    
    print(f"\nâœ“ æœºç»„{unit_id}ä»£ç†çº¦æŸè®­ç»ƒæµ‹è¯•å®Œæˆ", flush=True)
    return trainer


def evaluate_surrogate_effectiveness(trainer: SubproblemSurrogateTrainer, active_set_data: List[Dict]):
    """
    è¯„ä¼°ä»£ç†çº¦æŸçš„æœ‰æ•ˆæ€§
    
    æ¯”è¾ƒæœ‰æ— ä»£ç†çº¦æŸæ—¶çš„LPæ¾å¼›è§£è´¨é‡
    """
    g = trainer.unit_id
    T = trainer.T
    
    total_integrality_gap_without = 0.0
    total_integrality_gap_with = 0.0
    total_constraint_violation = 0.0
    target_feasibility_rate = 0.0
    
    n_test = min(5, len(active_set_data))
    
    for sample_id in range(n_test):
        lambda_val = trainer.lambda_vals[sample_id]
        alpha = trainer.alpha_values[sample_id]
        beta = trainer.beta_values[sample_id]
        
        # è·å–çœŸå®çš„æœºç»„çŠ¶æ€
        unit_commitment = active_set_data[sample_id].get('unit_commitment_matrix', None)
        if unit_commitment is not None and g < unit_commitment.shape[0]:
            x_target = unit_commitment[g]
        else:
            x_target = None
        
        # 1. æ— ä»£ç†çº¦æŸçš„LPæ¾å¼›
        x_without = solve_subproblem_LP_simple(trainer, sample_id, lambda_val, None, None)
        integrality_gap_without = np.sum(x_without * (1 - x_without))
        total_integrality_gap_without += integrality_gap_without
        
        # 2. æœ‰ä»£ç†çº¦æŸçš„LPæ¾å¼›
        x_with = solve_subproblem_LP_simple(trainer, sample_id, lambda_val, alpha, beta)
        integrality_gap_with = np.sum(x_with * (1 - x_with))
        total_integrality_gap_with += integrality_gap_with
        
        # 3. ä»£ç†çº¦æŸè¿åé‡
        constraint_viol = max(0, np.sum(alpha * x_with) - beta)
        total_constraint_violation += constraint_viol
        
        # 4. çœŸå®è§£çš„å¯è¡Œæ€§ï¼ˆä»£ç†çº¦æŸæ˜¯å¦ä¿ç•™çœŸå®è§£ï¼‰
        if x_target is not None:
            target_lhs = np.sum(alpha * x_target)
            if target_lhs <= beta + 1e-6:
                target_feasibility_rate += 1.0
        
        if sample_id < 3:
            print(f"\n  æ ·æœ¬ {sample_id}:", flush=True)
            print(f"    æ— ä»£ç†çº¦æŸæ•´æ•°æ€§é—´éš™: {integrality_gap_without:.4f}", flush=True)
            print(f"    æœ‰ä»£ç†çº¦æŸæ•´æ•°æ€§é—´éš™: {integrality_gap_with:.4f}", flush=True)
            print(f"    ä»£ç†çº¦æŸè¿åé‡: {constraint_viol:.6f}", flush=True)
            if x_target is not None:
                print(f"    çœŸå®è§£å¯è¡Œ: {target_lhs <= beta + 1e-6}", flush=True)
    
    avg_gap_without = total_integrality_gap_without / n_test
    avg_gap_with = total_integrality_gap_with / n_test
    avg_violation = total_constraint_violation / n_test
    feasibility_rate = target_feasibility_rate / n_test * 100
    
    print(f"\n  === æ€»ä½“è¯„ä¼° ===", flush=True)
    print(f"  å¹³å‡æ•´æ•°æ€§é—´éš™ (æ— ä»£ç†çº¦æŸ): {avg_gap_without:.4f}", flush=True)
    print(f"  å¹³å‡æ•´æ•°æ€§é—´éš™ (æœ‰ä»£ç†çº¦æŸ): {avg_gap_with:.4f}", flush=True)
    print(f"  é—´éš™å‡å°‘: {(avg_gap_without - avg_gap_with) / max(avg_gap_without, 1e-6) * 100:.2f}%", flush=True)
    print(f"  å¹³å‡ä»£ç†çº¦æŸè¿åé‡: {avg_violation:.6f}", flush=True)
    print(f"  çœŸå®è§£å¯è¡Œç‡: {feasibility_rate:.1f}%", flush=True)


def solve_subproblem_LP_simple(trainer: SubproblemSurrogateTrainer, sample_id: int,
                               lambda_val: np.ndarray, alpha: np.ndarray, beta: float) -> np.ndarray:
    """
    æ±‚è§£ç®€å•çš„å­é—®é¢˜LPæ¾å¼›
    
    Returns:
        xçš„LPæ¾å¼›è§£
    """
    g = trainer.unit_id
    T = trainer.T
    
    model = gp.Model('subproblem_LP_simple')
    model.Params.OutputFlag = 0
    
    pg = model.addVars(T, lb=0, name='pg')
    x = model.addVars(T, lb=0, ub=1, name='x')
    cpower = model.addVars(T, lb=0, name='cpower')
    
    # å‘ç”µä¸Šä¸‹é™çº¦æŸ
    for t in range(T):
        model.addConstr(pg[t] >= trainer.gen[g, PMIN] * x[t])
        model.addConstr(pg[t] <= trainer.gen[g, PMAX] * x[t])
    
    # çˆ¬å¡çº¦æŸ
    Ru = 0.4 * trainer.gen[g, PMAX] / trainer.T_delta
    Rd = 0.4 * trainer.gen[g, PMAX] / trainer.T_delta
    for t in range(1, T):
        model.addConstr(pg[t] - pg[t-1] <= Ru * x[t-1] + trainer.gen[g, PMAX] * (1 - x[t-1]))
        model.addConstr(pg[t-1] - pg[t] <= Rd * x[t] + trainer.gen[g, PMAX] * (1 - x[t]))
    
    # æœ€å°å¼€å…³æœºæ—¶é—´çº¦æŸ
    Ton = min(4, T)
    Toff = min(4, T)
    for tau in range(1, Ton+1):
        for t1 in range(T - tau):
            model.addConstr(x[t1+1] - x[t1] <= x[t1+tau])
    for tau in range(1, Toff+1):
        for t1 in range(T - tau):
            model.addConstr(-x[t1+1] + x[t1] <= 1 - x[t1+tau])
    
    # å‘ç”µæˆæœ¬
    for t in range(T):
        model.addConstr(cpower[t] >= trainer.gencost[g, -2]/trainer.T_delta * pg[t] + 
                      trainer.gencost[g, -1]/trainer.T_delta * x[t])
    
    # ä»£ç†çº¦æŸ
    if alpha is not None and beta is not None:
        model.addConstr(gp.quicksum(alpha[t] * x[t] for t in range(T)) <= beta)
    
    # ç›®æ ‡å‡½æ•°
    obj = gp.quicksum(cpower[t] for t in range(T))
    obj -= gp.quicksum(lambda_val[t] * pg[t] for t in range(T))
    
    model.setObjective(obj, GRB.MINIMIZE)
    model.optimize()
    
    if model.status == GRB.OPTIMAL:
        return np.array([x[t].X for t in range(T)])
    else:
        return np.zeros(T)


def test_multi_unit_surrogate(ppc=None, active_set_data=None, lambda_predictor=None,
                              unit_ids: List[int] = None, save_dir: str = None):
    """
    æµ‹è¯•å¤šæœºç»„ä»£ç†çº¦æŸè®­ç»ƒ
    
    Args:
        ppc: PyPoweræ¡ˆä¾‹æ•°æ®
        active_set_data: æ´»åŠ¨é›†æ•°æ®
        lambda_predictor: å·²è®­ç»ƒçš„å¯¹å¶å˜é‡é¢„æµ‹å™¨
        unit_ids: è¦è®­ç»ƒçš„æœºç»„IDåˆ—è¡¨
        save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        
    Returns:
        è®­ç»ƒå¥½çš„ä»£ç†çº¦æŸè®­ç»ƒå™¨å­—å…¸
    """
    if not PYPOWER_AVAILABLE:
        print("pypoweræœªå®‰è£…ï¼Œè·³è¿‡æµ‹è¯•", flush=True)
        return None
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•3: å¤šæœºç»„ä»£ç†çº¦æŸè®­ç»ƒ")
    print("=" * 60)
    
    # å‡†å¤‡æ•°æ®
    if ppc is None:
        ppc = pypower.case30.case30()
    
    ppc_int = ext2int(ppc)
    ng = ppc_int['gen'].shape[0]
    
    T = 8
    if active_set_data is None:
        active_set_data = generate_test_data(ppc, T=T, n_samples=15)
    
    if unit_ids is None:
        unit_ids = list(range(min(3, ng)))  # é»˜è®¤è®­ç»ƒå‰3ä¸ªæœºç»„
    
    trainers = {}
    
    for g in unit_ids:
        print(f"\n--- æœºç»„ {g} ---", flush=True)
        
        trainer = SubproblemSurrogateTrainer(
            ppc, active_set_data, T_delta=1.0, unit_id=g,
            lambda_predictor=lambda_predictor
        )
        
        trainer.iter(max_iter=10, nn_epochs=5)
        trainers[g] = trainer
        
        if save_dir:
            os.makedirs(save_dir, exist_ok=True)
            trainer.save(os.path.join(save_dir, f'surrogate_unit_{g}.pth'))
    
    print(f"\nâœ“ å¤šæœºç»„ä»£ç†çº¦æŸè®­ç»ƒå®Œæˆ ({len(unit_ids)} ä¸ªæœºç»„)", flush=True)
    return trainers


def test_save_load(ppc=None, active_set_data=None):
    """
    æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½
    """
    if not PYPOWER_AVAILABLE or not TORCH_AVAILABLE:
        print("ä¾èµ–æœªå®‰è£…ï¼Œè·³è¿‡æµ‹è¯•", flush=True)
        return
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•4: æ¨¡å‹ä¿å­˜å’ŒåŠ è½½")
    print("=" * 60)
    
    # å‡†å¤‡æ•°æ®
    if ppc is None:
        ppc = pypower.case30.case30()
    
    T = 8
    if active_set_data is None:
        active_set_data = generate_test_data(ppc, T=T, n_samples=10)
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    import tempfile
    temp_dir = tempfile.mkdtemp()
    
    try:
        # 1. è®­ç»ƒå¹¶ä¿å­˜å¯¹å¶é¢„æµ‹å™¨
        print("\n--- è®­ç»ƒå¯¹å¶é¢„æµ‹å™¨ ---", flush=True)
        predictor = DualVariablePredictorTrainer(ppc, active_set_data, T_delta=1.0)
        predictor.train(num_epochs=30)
        
        dual_path = os.path.join(temp_dir, 'dual_predictor.pth')
        predictor.save(dual_path)
        
        # 2. åŠ è½½å¯¹å¶é¢„æµ‹å™¨å¹¶éªŒè¯
        print("\n--- åŠ è½½å¹¶éªŒè¯å¯¹å¶é¢„æµ‹å™¨ ---", flush=True)
        predictor2 = DualVariablePredictorTrainer(ppc, active_set_data, T_delta=1.0)
        predictor2.load(dual_path)
        
        # éªŒè¯é¢„æµ‹ç»“æœä¸€è‡´
        test_pd = active_set_data[0]['pd_data']
        pred1 = predictor.predict(test_pd)
        pred2 = predictor2.predict(test_pd)
        diff = np.max(np.abs(pred1 - pred2))
        print(f"  å¯¹å¶é¢„æµ‹å™¨åŠ è½½éªŒè¯: æœ€å¤§å·®å¼‚ = {diff:.8f}", flush=True)
        assert diff < 1e-5, "å¯¹å¶é¢„æµ‹å™¨åŠ è½½å¤±è´¥"
        
        # 3. è®­ç»ƒå¹¶ä¿å­˜ä»£ç†çº¦æŸ
        print("\n--- è®­ç»ƒä»£ç†çº¦æŸ ---", flush=True)
        trainer = SubproblemSurrogateTrainer(
            ppc, active_set_data, T_delta=1.0, unit_id=0,
            lambda_predictor=predictor
        )
        trainer.iter(max_iter=5, nn_epochs=3)
        
        surrogate_path = os.path.join(temp_dir, 'surrogate_unit_0.pth')
        trainer.save(surrogate_path)
        
        # 4. åŠ è½½ä»£ç†çº¦æŸå¹¶éªŒè¯
        print("\n--- åŠ è½½å¹¶éªŒè¯ä»£ç†çº¦æŸ ---", flush=True)
        trainer2 = SubproblemSurrogateTrainer(
            ppc, active_set_data, T_delta=1.0, unit_id=0,
            lambda_predictor=predictor
        )
        trainer2.load(surrogate_path)
        
        # éªŒè¯ä»£ç†çº¦æŸå‚æ•°ä¸€è‡´
        alpha1, beta1 = trainer.get_surrogate_params(test_pd, trainer.lambda_vals[0])
        alpha2, beta2 = trainer2.get_surrogate_params(test_pd, trainer2.lambda_vals[0])
        diff_alpha = np.max(np.abs(alpha1 - alpha2))
        diff_beta = abs(beta1 - beta2)
        print(f"  ä»£ç†çº¦æŸåŠ è½½éªŒè¯: alphaå·®å¼‚ = {diff_alpha:.8f}, betaå·®å¼‚ = {diff_beta:.8f}", flush=True)
        assert diff_alpha < 1e-5 and diff_beta < 1e-5, "ä»£ç†çº¦æŸåŠ è½½å¤±è´¥"
        
        print("\nâœ“ æ¨¡å‹ä¿å­˜å’ŒåŠ è½½æµ‹è¯•é€šè¿‡", flush=True)
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)


def test_end_to_end(case_name: str = 'case30', n_samples: int = 20, 
                    num_units: int = 3, save_dir: str = None):
    """
    ç«¯åˆ°ç«¯å®Œæ•´æµ‹è¯•
    
    Args:
        case_name: PyPoweræ¡ˆä¾‹åç§° ('case14', 'case30', 'case39')
        n_samples: æ ·æœ¬æ•°é‡
        num_units: è®­ç»ƒçš„æœºç»„æ•°é‡
        save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
    """
    if not PYPOWER_AVAILABLE or not TORCH_AVAILABLE:
        print("ä¾èµ–æœªå®‰è£…ï¼Œè·³è¿‡æµ‹è¯•", flush=True)
        return
    
    print("\n" + "=" * 60)
    print(f"ç«¯åˆ°ç«¯å®Œæ•´æµ‹è¯• ({case_name}, {n_samples}æ ·æœ¬, {num_units}æœºç»„)")
    print("=" * 60)
    
    # 1. åŠ è½½æ¡ˆä¾‹
    if case_name == 'case14':
        ppc = pypower.case14.case14()
    elif case_name == 'case30':
        ppc = pypower.case30.case30()
    elif case_name == 'case39':
        ppc = pypower.case39.case39()
    else:
        print(f"æœªçŸ¥æ¡ˆä¾‹: {case_name}", flush=True)
        return
    
    ppc_int = ext2int(ppc)
    ng = ppc_int['gen'].shape[0]
    num_units = min(num_units, ng)
    
    # 2. ç”Ÿæˆæ•°æ®
    print("\nã€æ­¥éª¤1ã€‘ç”Ÿæˆæµ‹è¯•æ•°æ®", flush=True)
    active_set_data = generate_test_data(ppc, T=8, n_samples=n_samples)
    
    # 3. è®­ç»ƒå¯¹å¶é¢„æµ‹å™¨
    print("\nã€æ­¥éª¤2ã€‘è®­ç»ƒå¯¹å¶å˜é‡é¢„æµ‹å™¨", flush=True)
    dual_predictor = DualVariablePredictorTrainer(ppc, active_set_data, T_delta=1.0)
    dual_predictor.train(num_epochs=100)
    
    # 4. è®­ç»ƒå¤šæœºç»„ä»£ç†çº¦æŸ
    print("\nã€æ­¥éª¤3ã€‘è®­ç»ƒå¤šæœºç»„ä»£ç†çº¦æŸ", flush=True)
    trainers = {}
    for g in range(num_units):
        print(f"\n  --- æœºç»„ {g}/{num_units-1} ---", flush=True)
        trainer = SubproblemSurrogateTrainer(
            ppc, active_set_data, T_delta=1.0, unit_id=g,
            lambda_predictor=dual_predictor
        )
        trainer.iter(max_iter=15, nn_epochs=8)
        trainers[g] = trainer
    
    # 5. è¯„ä¼°æ•´ä½“æ•ˆæœ
    print("\nã€æ­¥éª¤4ã€‘æ•´ä½“æ•ˆæœè¯„ä¼°", flush=True)
    total_gap_reduction = 0.0
    total_feasibility = 0.0
    
    for g, trainer in trainers.items():
        print(f"\n  æœºç»„ {g}:", flush=True)
        
        gap_without_sum = 0.0
        gap_with_sum = 0.0
        feasible_count = 0
        
        for sample_id in range(min(5, n_samples)):
            lambda_val = trainer.lambda_vals[sample_id]
            alpha = trainer.alpha_values[sample_id]
            beta = trainer.beta_values[sample_id]
            
            x_without = solve_subproblem_LP_simple(trainer, sample_id, lambda_val, None, None)
            x_with = solve_subproblem_LP_simple(trainer, sample_id, lambda_val, alpha, beta)
            
            gap_without = np.sum(x_without * (1 - x_without))
            gap_with = np.sum(x_with * (1 - x_with))
            
            gap_without_sum += gap_without
            gap_with_sum += gap_with
            
            # æ£€æŸ¥çœŸå®è§£å¯è¡Œæ€§
            unit_commitment = active_set_data[sample_id].get('unit_commitment_matrix', None)
            if unit_commitment is not None and g < unit_commitment.shape[0]:
                x_target = unit_commitment[g]
                if np.sum(alpha * x_target) <= beta + 1e-6:
                    feasible_count += 1
        
        n_test = min(5, n_samples)
        avg_gap_without = gap_without_sum / n_test
        avg_gap_with = gap_with_sum / n_test
        gap_reduction = (avg_gap_without - avg_gap_with) / max(avg_gap_without, 1e-6) * 100
        feasibility_rate = feasible_count / n_test * 100
        
        print(f"    æ•´æ•°æ€§é—´éš™å‡å°‘: {gap_reduction:.2f}%", flush=True)
        print(f"    çœŸå®è§£å¯è¡Œç‡: {feasibility_rate:.1f}%", flush=True)
        
        total_gap_reduction += gap_reduction
        total_feasibility += feasibility_rate
    
    print(f"\n  === å¹³å‡ç»“æœ ===", flush=True)
    print(f"  å¹³å‡æ•´æ•°æ€§é—´éš™å‡å°‘: {total_gap_reduction / num_units:.2f}%", flush=True)
    print(f"  å¹³å‡çœŸå®è§£å¯è¡Œç‡: {total_feasibility / num_units:.1f}%", flush=True)
    
    # 6. ä¿å­˜æ¨¡å‹
    if save_dir:
        print(f"\nã€æ­¥éª¤5ã€‘ä¿å­˜æ¨¡å‹åˆ° {save_dir}", flush=True)
        os.makedirs(save_dir, exist_ok=True)
        
        dual_predictor.save(os.path.join(save_dir, 'dual_predictor.pth'))
        for g, trainer in trainers.items():
            trainer.save(os.path.join(save_dir, f'surrogate_unit_{g}.pth'))
        
        print("âœ“ æ¨¡å‹ä¿å­˜å®Œæˆ", flush=True)
    
    print("\n" + "=" * 60)
    print("ç«¯åˆ°ç«¯æµ‹è¯•å®Œæˆ!")
    print("=" * 60)
    
    return dual_predictor, trainers


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("å­ä»£ç†æ¨¡å‹è®­ç»ƒæ¨¡å—")
    print("=" * 60)
    
    if not TORCH_AVAILABLE:
        print("é”™è¯¯: PyTorchæœªå®‰è£…", flush=True)
        return
    
    if not PYPOWER_AVAILABLE:
        print("é”™è¯¯: pypoweræœªå®‰è£…", flush=True)
        return
    
    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    print("\nå¯ç”¨æ¨¡å¼:")
    print("  === è®­ç»ƒæ¨¡å¼ ===")
    print("  1. å®Œæ•´è®­ç»ƒ (å¯¹å¶é¢„æµ‹å™¨ + æ‰€æœ‰æœºç»„ä»£ç†çº¦æŸ)")
    print("  2. ä»…è®­ç»ƒå¯¹å¶å˜é‡é¢„æµ‹å™¨")
    print("  3. ä»…è®­ç»ƒæŒ‡å®šæœºç»„ä»£ç†çº¦æŸ")
    print("  === æµ‹è¯•æ¨¡å¼ ===")
    print("  4. å¯¹å¶å˜é‡é¢„æµ‹å™¨æµ‹è¯•")
    print("  5. å•æœºç»„ä»£ç†çº¦æŸæµ‹è¯•")
    print("  6. å¤šæœºç»„ä»£ç†çº¦æŸæµ‹è¯•")
    print("  7. æ¨¡å‹ä¿å­˜/åŠ è½½æµ‹è¯•")
    print("  8. ç«¯åˆ°ç«¯å®Œæ•´æµ‹è¯•")
    print("  9. è¿è¡Œæ‰€æœ‰æµ‹è¯•")
    
    # é»˜è®¤è¿è¡Œå®Œæ•´è®­ç»ƒ
    mode = 1
    
    # è®¡ç®—é¡¹ç›®æ ¹ç›®å½•ï¼ˆåŸºäºå½“å‰è„šæœ¬ä½ç½®ï¼‰
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)  # srcçš„çˆ¶ç›®å½•å³ä¸ºé¡¹ç›®æ ¹ç›®å½•
    result_dir = os.path.join(project_root, 'result', 'subproblem_models')
    
    # #region agent log
    import json as _json_debug; _log_path = r'd:\0-python_workspace\branchandcut\.cursor\debug.log'; _log_data = {"location": "uc_NN_subproblem.py:main:2147", "message": "Path calculation", "data": {"script_dir": script_dir, "project_root": project_root, "result_dir": result_dir, "cwd": os.getcwd()}, "timestamp": int(__import__('time').time()*1000), "sessionId": "debug-session", "hypothesisId": "D"}; open(_log_path, 'a', encoding='utf-8').write(_json_debug.dumps(_log_data) + '\n')
    # #endregion
    
    # ==================== è®­ç»ƒæ¨¡å¼ ====================
    if mode == 1:
        # å®Œæ•´è®­ç»ƒ
        print("\n>>> å®Œæ•´è®­ç»ƒæ¨¡å¼ <<<\n")
        
        # é…ç½®å‚æ•°
        case_name = 'case30'  # å¯é€‰: 'case14', 'case30', 'case39'
        n_samples = 20
        T = 8
        T_delta = 1.0
        unit_ids = None  # Noneè¡¨ç¤ºæ‰€æœ‰æœºç»„ï¼Œæˆ–æŒ‡å®šå¦‚ [0, 1, 2]
        save_dir = result_dir  # ä½¿ç”¨ç»å¯¹è·¯å¾„
        
        # è®­ç»ƒå‚æ•°
        dual_epochs = 100
        dual_batch_size = 8
        surrogate_max_iter = 20
        surrogate_nn_epochs = 10
        
        # åŠ è½½æ¡ˆä¾‹
        if case_name == 'case14':
            ppc = pypower.case14.case14()
        elif case_name == 'case30':
            ppc = pypower.case30.case30()
        elif case_name == 'case39':
            ppc = pypower.case39.case39()
        else:
            print(f"æœªçŸ¥æ¡ˆä¾‹: {case_name}")
            return
        
        # ç”Ÿæˆè®­ç»ƒæ•°æ®
        active_set_data = generate_test_data(ppc, T=T, n_samples=n_samples)
        
        # å®Œæ•´è®­ç»ƒ
        dual_predictor, trainers = train_complete_model(
            ppc, active_set_data, T_delta,
            unit_ids=unit_ids,
            dual_epochs=dual_epochs,
            dual_batch_size=dual_batch_size,
            surrogate_max_iter=surrogate_max_iter,
            surrogate_nn_epochs=surrogate_nn_epochs,
            save_dir=save_dir
        )
        
        # è¯„ä¼°æ¨¡å‹
        evaluate_trained_models(dual_predictor, trainers, active_set_data)
        
    elif mode == 2:
        # ä»…è®­ç»ƒå¯¹å¶é¢„æµ‹å™¨
        print("\n>>> ä»…è®­ç»ƒå¯¹å¶å˜é‡é¢„æµ‹å™¨ <<<\n")
        
        ppc = pypower.case30.case30()
        active_set_data = generate_test_data(ppc, T=8, n_samples=20)
        
        predictor = train_dual_predictor_from_data(
            ppc, active_set_data, T_delta=1.0,
            num_epochs=100, batch_size=8,
            save_path=os.path.join(result_dir, 'dual_predictor.pth')
        )
        
    elif mode == 3:
        # ä»…è®­ç»ƒæŒ‡å®šæœºç»„ä»£ç†çº¦æŸ
        print("\n>>> ä»…è®­ç»ƒæŒ‡å®šæœºç»„ä»£ç†çº¦æŸ <<<\n")
        
        ppc = pypower.case30.case30()
        active_set_data = generate_test_data(ppc, T=8, n_samples=20)
        
        # å…ˆè®­ç»ƒå¯¹å¶é¢„æµ‹å™¨
        predictor = train_dual_predictor_from_data(
            ppc, active_set_data, T_delta=1.0, num_epochs=100
        )
        
        # è®­ç»ƒæŒ‡å®šæœºç»„
        unit_id = 0
        trainer = train_subproblem_surrogate_from_data(
            ppc, active_set_data, unit_id=unit_id, T_delta=1.0,
            lambda_predictor=predictor,
            max_iter=20, nn_epochs=10,
            save_path=os.path.join(result_dir, f'surrogate_unit_{unit_id}.pth')
        )
    
    # ==================== æµ‹è¯•æ¨¡å¼ ====================
    elif mode == 4:
        test_dual_predictor()
        
    elif mode == 5:
        predictor = test_dual_predictor()
        test_subproblem_surrogate(lambda_predictor=predictor)
        
    elif mode == 6:
        predictor = test_dual_predictor()
        test_multi_unit_surrogate(lambda_predictor=predictor)
        
    elif mode == 7:
        test_save_load()
        
    elif mode == 8:
        test_end_to_end(case_name='case30', n_samples=15, num_units=3)
        
    elif mode == 9:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        print("\n>>> è¿è¡Œæ‰€æœ‰æµ‹è¯• <<<\n")
        
        # ç”Ÿæˆå…±äº«æ•°æ®
        ppc = pypower.case30.case30()
        active_set_data = generate_test_data(ppc, T=8, n_samples=15)
        
        # æµ‹è¯•1: å¯¹å¶é¢„æµ‹å™¨
        predictor = test_dual_predictor(ppc, active_set_data)
        
        # æµ‹è¯•2: å•æœºç»„ä»£ç†çº¦æŸ
        test_subproblem_surrogate(ppc, active_set_data, predictor, unit_id=0)
        
        # æµ‹è¯•3: å¤šæœºç»„ä»£ç†çº¦æŸ
        test_multi_unit_surrogate(ppc, active_set_data, predictor, unit_ids=[0, 1])
        
        # æµ‹è¯•4: ä¿å­˜/åŠ è½½
        test_save_load(ppc, active_set_data)
        
        print("\n" + "=" * 60)
        print("æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print("=" * 60)
    
    else:
        print(f"æœªçŸ¥æ¨¡å¼: {mode}")


def run_training(case_name: str = 'case30', n_samples: int = 20, T: int = 8,
                 unit_ids: List[int] = None, save_dir: str = '../result/subproblem_models',
                 dual_epochs: int = 100, surrogate_max_iter: int = 20,
                 surrogate_nn_epochs: int = 10):
    """
    ä¾¿æ·çš„è®­ç»ƒå…¥å£å‡½æ•°
    
    Args:
        case_name: PyPoweræ¡ˆä¾‹åç§° ('case14', 'case30', 'case39')
        n_samples: æ ·æœ¬æ•°é‡
        T: æ—¶æ®µæ•°
        unit_ids: è¦è®­ç»ƒçš„æœºç»„IDåˆ—è¡¨ï¼ˆNoneè¡¨ç¤ºæ‰€æœ‰æœºç»„ï¼‰
        save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        dual_epochs: å¯¹å¶é¢„æµ‹å™¨è®­ç»ƒè½®æ•°
        surrogate_max_iter: ä»£ç†çº¦æŸBCDè¿­ä»£æ¬¡æ•°
        surrogate_nn_epochs: ä»£ç†çº¦æŸNNè®­ç»ƒè½®æ•°
        
    Returns:
        (dual_predictor, trainers) å…ƒç»„
    """
    # åŠ è½½æ¡ˆä¾‹
    if case_name == 'case14':
        ppc = pypower.case14.case14()
    elif case_name == 'case30':
        ppc = pypower.case30.case30()
    elif case_name == 'case39':
        ppc = pypower.case39.case39()
    else:
        raise ValueError(f"æœªçŸ¥æ¡ˆä¾‹: {case_name}")
    
    # ç”Ÿæˆæ•°æ®
    active_set_data = generate_test_data(ppc, T=T, n_samples=n_samples)
    
    # è®­ç»ƒæ¨¡å‹
    dual_predictor, trainers = train_complete_model(
        ppc, active_set_data, T_delta=1.0,
        unit_ids=unit_ids,
        dual_epochs=dual_epochs,
        surrogate_max_iter=surrogate_max_iter,
        surrogate_nn_epochs=surrogate_nn_epochs,
        save_dir=save_dir
    )
    
    # è¯„ä¼°æ¨¡å‹
    evaluate_trained_models(dual_predictor, trainers, active_set_data)
    
    return dual_predictor, trainers


if __name__ == "__main__":
    main()
